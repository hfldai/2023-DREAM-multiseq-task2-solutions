{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hfldai/2023-DREAM-multiseq-task2-solutions/blob/main/broad-1/notebooks/broad1_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPFmKWEDygR6"
      },
      "source": [
        "![Cover](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/broad-1/assets/cover.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8PyJPSkxoRo"
      },
      "source": [
        "## Autoimmune Disease Machine Learning Crunch #1\n",
        "\n",
        "## Quickstarter: Resnet50 + Ridge\n",
        "\n",
        "This is an example submission and can be used as a starting point, but should not be seen as the only way to approach the problem. However, this approach can be improved in many ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovnPAKW52GYm"
      },
      "source": [
        "This notebook addresses the task of **predicting gene expression** in spatial transcriptomics data of colon tissue using H&E pathology images. The goal is to predict the expression of **460 genes** in tissue patches based on H&E images and corresponding spatial transcriptomics data.\n",
        "\n",
        "The solution follows a two-step process:\n",
        "\n",
        "- **ResNet50 Embedding**: We use the pre-trained ResNet50 model, trained on ImageNet, to extract features from the H&E images. The model transforms the high-dimensional image data into a lower-dimensional numerical representation (embedding vector).\n",
        "- **Ridge Regression**: After obtaining the embedding vectors, ridge regression is applied to predict the gene expression profiles for each nucleus/cell.\n",
        "\n",
        "> For CrunchDAO Challenge submissions, where internet access is restricted, you will need to **pre-download** the model file `pytorch_model.bin` from its [Resnet50 Hugging Face](https://huggingface.co/timm/resnet50.tv_in1k/tree/main) card (link: https://huggingface.co/timm/resnet50.tv_in1k/tree/main) and submit it along with the notebook [(see how submit below)](#scrollTo=kppG1inFIopT&line=8&uniqifier=1).\n",
        "\n",
        "This code is adapted from the GitHub repository [HEST-library](https://github.com/mahmoodlab/hest) (link: https://github.com/mahmoodlab/hest). Dive into the repository to explore advanced pre-trained embedding models, experiment with alternative regression techniques or tap into external datasets to push the boundaries of performance!\n",
        "\n",
        "For exploring other pre-trained models, you can also visit:\n",
        "\n",
        "- Hugging Face Timm: https://huggingface.co/timm\n",
        "- PyTorch Vision Models: https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "Note: This notebook is designed to run seamlessly on a **CPU** environment. (25 min for train step with default values - Mean Squared Error of ~0.10 and Pearson correlation coefficient of ~0.15 on unseen H&E data) -- (1h for submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQHlrgiNekV"
      },
      "source": [
        "`Predicting spatial transcriptomics data from an H&E image (Crunch 1)`\n",
        "\n",
        "![crunch_1_challenge](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/crunch_1_challenge.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UjSFajKnBnq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKVOi6J1loBn"
      },
      "source": [
        "You need a **token** to load the dataset of the challenge.\n",
        "\n",
        "Get a new token: https://hub.crunchdao.com/competitions/broad-1/submit/via/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9pR4gsI9VE2",
        "outputId": "6439d9b0-eba3-4ee3-8f9a-5f5bc69fd080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crunch-cli\n",
            "  Downloading crunch_cli-5.7.5-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.1.7)\n",
            "Collecting coloredlogs (from crunch-cli)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses_json (from crunch-cli)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.5.0)\n",
            "Collecting inquirer (from crunch-cli)\n",
            "  Downloading inquirer-3.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.4.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (3.4.2)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (24.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (17.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (8.3.4)\n",
            "Collecting python-dotenv (from crunch-cli)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (6.0.2)\n",
            "Collecting redbaron (from crunch-cli)\n",
            "  Downloading redbaron-0.9.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.0.0)\n",
            "Requirement already satisfied: requirements-parser in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from crunch-cli) (4.67.1)\n",
            "Collecting spatialdata-plot (from crunch-cli)\n",
            "  Downloading spatialdata_plot-0.2.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->crunch-cli)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json->crunch-cli)\n",
            "  Downloading marshmallow-3.23.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json->crunch-cli)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata->crunch-cli) (3.21.0)\n",
            "Collecting blessed>=1.19.0 (from inquirer->crunch-cli)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting editor>=1.6.0 (from inquirer->crunch-cli)\n",
            "  Downloading editor-1.6.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting readchar>=4.2.0 (from inquirer->crunch-cli)\n",
            "  Downloading readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->crunch-cli) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->crunch-cli) (2.2.1)\n",
            "Collecting baron>=0.7 (from redbaron->crunch-cli)\n",
            "  Downloading baron-0.10.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->crunch-cli) (2024.12.14)\n",
            "Requirement already satisfied: types-setuptools>=69.1.0 in /usr/local/lib/python3.10/dist-packages (from requirements-parser->crunch-cli) (75.6.0.20241223)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->crunch-cli) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spatialdata-plot->crunch-cli) (3.8.0)\n",
            "Collecting matplotlib-scalebar (from spatialdata-plot->crunch-cli)\n",
            "  Downloading matplotlib_scalebar-0.8.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting scanpy (from spatialdata-plot->crunch-cli)\n",
            "  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting spatialdata>=0.2.6 (from spatialdata-plot->crunch-cli)\n",
            "  Downloading spatialdata-0.2.6-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting rply (from baron>=0.7->redbaron->crunch-cli)\n",
            "  Downloading rply-0.7.8-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer->crunch-cli) (0.2.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.17.0)\n",
            "Collecting runs (from editor>=1.6.0->inquirer->crunch-cli)\n",
            "  Downloading runs-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xmod (from editor>=1.6.0->inquirer->crunch-cli)\n",
            "  Downloading xmod-1.8.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting anndata>=0.9.1 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading anndata-0.11.1-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting dask-image (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading dask_image-2024.5.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: dask>=2024.4.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.10.0)\n",
            "Collecting fsspec<=2023.6 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: geopandas>=0.14 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.1)\n",
            "Collecting multiscale-spatial-image>=2.0.2 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading multiscale_spatial_image-2.0.2-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.60.0)\n",
            "Collecting ome-zarr>=0.8.4 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading ome_zarr-0.10.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.8.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (13.9.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (75.1.0)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.0.6)\n",
            "Collecting spatial-image>=1.1.0 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading spatial_image-1.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.12.2)\n",
            "Collecting xarray-schema (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading xarray_schema-0.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting xarray-spatial>=0.3.5 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading xarray_spatial-0.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: xarray>=2024.10.0 in /usr/local/lib/python3.10/dist-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.11.0)\n",
            "Collecting zarr<3 (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json->crunch-cli)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spatialdata-plot->crunch-cli) (3.2.0)\n",
            "Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (3.12.1)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy->spatialdata-plot->crunch-cli)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (8.4.0)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (1.0.1)\n",
            "Collecting pynndescent>=0.5 (from scanpy->spatialdata-plot->crunch-cli)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (0.13.2)\n",
            "Collecting session-info (from scanpy->spatialdata-plot->crunch-cli)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->spatialdata-plot->crunch-cli) (0.14.4)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy->spatialdata-plot->crunch-cli)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.9.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading array_api_compat-1.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.12.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.10.0)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.7.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.43.0)\n",
            "Collecting distributed (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading distributed-2024.12.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.11.10)\n",
            "Collecting xarray-dataclasses>=1.1.0 (from spatial-image>=1.1.0->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading xarray_dataclasses-1.9.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datashader>=0.15.0 (from xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading datashader-0.16.3-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting asciitree (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numcodecs>=0.10.0 (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting fasteners (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pims>=0.4.1 (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading pims-0.7.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tifffile>=2018.10.18 in /usr/local/lib/python3.10/dist-packages (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.12.12)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.3.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.18.0)\n",
            "Collecting appdirs (from rply->baron>=0.7->redbaron->crunch-cli)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.36.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.4)\n",
            "Collecting stdlib_list (from session-info->scanpy->spatialdata-plot->crunch-cli)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.18.3)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.2.0)\n",
            "Collecting pyct (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.1.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
            "Collecting slicerator>=0.9.8 (from pims>=0.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading slicerator-1.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is looking at multiple versions of distributed to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting distributed (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading distributed-2024.12.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading distributed-2024.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading distributed-2024.11.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading distributed-2024.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading distributed-2024.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.4)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.0)\n",
            "Collecting sortedcontainers>=2.0.5 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tblib>=1.6.0 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.3.3)\n",
            "Collecting zict>=3.0.0 (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.2)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading aiobotocore-2.16.1-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.7.0 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting aiobotocore~=2.5.4 (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting s3fs (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.17.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.4->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->crunch-cli)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crunch_cli-5.7.5-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading inquirer-3.4.0-py3-none-any.whl (18 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading redbaron-0.9.2-py2.py3-none-any.whl (34 kB)\n",
            "Downloading spatialdata_plot-0.2.8-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading baron-0.10.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editor-1.6.6-py3-none-any.whl (4.0 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
            "Downloading spatialdata-0.2.6-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.7/168.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading matplotlib_scalebar-0.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.1-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading multiscale_spatial_image-2.0.2-py3-none-any.whl (29 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading ome_zarr-0.10.2-py3-none-any.whl (37 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spatial_image-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray_spatial-0.4.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_image-2024.5.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rply-0.7.8-py2.py3-none-any.whl (16 kB)\n",
            "Downloading runs-1.2.2-py3-none-any.whl (7.0 kB)\n",
            "Downloading xarray_schema-0.0.3-py3-none-any.whl (10 kB)\n",
            "Downloading xmod-1.8.1-py3-none-any.whl (4.6 kB)\n",
            "Downloading array_api_compat-1.10.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datashader-0.16.3-py2.py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray_dataclasses-1.9.1-py3-none-any.whl (14 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading distributed-2024.10.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicerator-1.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: session-info, pims, asciitree\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=80a78f4dbbfc528fcaad1781413fc98cab2578dd2d1b070437366319b700fb17\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "  Building wheel for pims (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pims: filename=PIMS-0.7-py3-none-any.whl size=84591 sha256=ddd1790353971d66d1f1cfd1ae73cbf0fa9750bb2f65c4db1e638fa3ecf19594\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/cb/f1/939f4adc0c5bcb1a1a78566d67869368d3d8dc8abd84f63c38\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=f7c7b39c47485ac464c205e946b3dcf7a57f84231ff4f257b9babee502571db9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
            "Successfully built session-info pims asciitree\n",
            "Installing collected packages: sortedcontainers, slicerator, asciitree, appdirs, zict, xmod, urllib3, tblib, stdlib_list, rply, readchar, python-dotenv, pyct, numcodecs, mypy-extensions, marshmallow, legacy-api-wrap, jmespath, humanfriendly, fsspec, fasteners, blessed, array-api-compat, aioitertools, zarr, typing-inspect, session-info, runs, pims, coloredlogs, botocore, baron, redbaron, pynndescent, matplotlib-scalebar, editor, distributed, dataclasses_json, dask-expr, anndata, xarray-schema, xarray-dataclasses, umap-learn, inquirer, datashader, aiobotocore, xarray-spatial, spatial-image, scanpy, s3fs, dask-image, multiscale-spatial-image, ome-zarr, spatialdata, spatialdata-plot, crunch-cli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.5.4 aioitertools-0.12.0 anndata-0.11.1 appdirs-1.4.4 array-api-compat-1.10.0 asciitree-0.3.3 baron-0.10.1 blessed-1.20.0 botocore-1.31.17 coloredlogs-15.0.1 crunch-cli-5.7.5 dask-expr-1.1.16 dask-image-2024.5.3 dataclasses_json-0.6.7 datashader-0.16.3 distributed-2024.10.0 editor-1.6.6 fasteners-0.19 fsspec-2023.6.0 humanfriendly-10.0 inquirer-3.4.0 jmespath-1.0.1 legacy-api-wrap-1.4.1 marshmallow-3.23.3 matplotlib-scalebar-0.8.1 multiscale-spatial-image-2.0.2 mypy-extensions-1.0.0 numcodecs-0.13.1 ome-zarr-0.10.2 pims-0.7 pyct-0.5.0 pynndescent-0.5.13 python-dotenv-1.0.1 readchar-4.2.1 redbaron-0.9.2 rply-0.7.8 runs-1.2.2 s3fs-2023.6.0 scanpy-1.10.4 session-info-1.0.0 slicerator-1.1.0 sortedcontainers-2.4.0 spatial-image-1.1.0 spatialdata-0.2.6 spatialdata-plot-0.2.8 stdlib_list-0.11.0 tblib-3.0.0 typing-inspect-0.9.0 umap-learn-0.5.7 urllib3-1.26.20 xarray-dataclasses-1.9.1 xarray-schema-0.0.3 xarray-spatial-0.4.0 xmod-1.8.1 zarr-2.18.3 zict-3.0.0\n",
            "main.py: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15828/main.py (64856 bytes)\n",
            "notebook.ipynb: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15828/notebook.ipynb (1449547 bytes)\n",
            "requirements.txt: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/15828/requirements.original.txt (296 bytes)\n",
            "resources/pytorch_model.bin: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/models/16958/pytorch_model.bin (102545229 bytes)\n",
            "data/DC1.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/DC1.zarr.zip (720810785 bytes)\n",
            "data/DC1.zarr.zip: uncompress into data/DC1.zarr.zip.6ao5cruq\n",
            "data/UC1_NI.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC1_NI.zarr.zip (889729934 bytes)\n",
            "data/UC1_NI.zarr.zip: uncompress into data/UC1_NI.zarr.zip.tys70xfa\n",
            "data/UC6_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC6_I.zarr.zip (923581852 bytes)\n",
            "data/UC6_I.zarr.zip: uncompress into data/UC6_I.zarr.zip.nhzfj62b\n",
            "data/UC7_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC7_I.zarr.zip (769304631 bytes)\n",
            "data/UC7_I.zarr.zip: uncompress into data/UC7_I.zarr.zip.4syric22\n",
            "data/UC6_NI.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC6_NI.zarr.zip (706148035 bytes)\n",
            "data/UC6_NI.zarr.zip: uncompress into data/UC6_NI.zarr.zip.3ndy34zi\n",
            "data/UC1_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC1_I.zarr.zip (926772130 bytes)\n",
            "data/UC1_I.zarr.zip: uncompress into data/UC1_I.zarr.zip.bdarjend\n",
            "data/UC9_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/UC9_I.zarr.zip (927207232 bytes)\n",
            "data/UC9_I.zarr.zip: uncompress into data/UC9_I.zarr.zip.03eby7mb\n",
            "data/DC5.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/81/DC5.zarr.zip (813303073 bytes)\n",
            "data/DC5.zarr.zip: uncompress into data/DC5.zarr.zip.xvbj3by5\n",
            "\n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            " - To see all of the available commands of the CrunchDAO CLI, run: crunch --help\n"
          ]
        }
      ],
      "source": [
        "%pip install crunch-cli --upgrade\n",
        "!crunch setup --notebook --size default broad-1 test-platform --token TZTCBvtNxSPJldy2Exx52n3ivOJfXixPonhL6iKzB78z5AlHaRMFdJ0ySmCMfmdS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4gfY7jC5f-n"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAzquCA99X33"
      },
      "outputs": [],
      "source": [
        "import crunch\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0b6AgVirxijj"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas torch scipy openslide-python pydantic pytorch-lightning dask distributed matplotlib seaborn scikit-learn opencv-python scanpy spatialdata zarr ome-zarr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vd1p0CnExnj5",
        "outputId": "774b4b92-7a2f-420d-c207-4aa7c23c73bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
            "\n",
            "stdout:\n",
            "\n",
            "\n",
            "\n",
            "stderr:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 295, in __getattr__\n",
            "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
            "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
            "\n",
            "CUDA driver library cannot be found.\n",
            "If you are sure that a CUDA driver is installed,\n",
            "try setting environment variable NUMBA_CUDA_DRIVER\n",
            "with the file path of the CUDA driver shared library.\n",
            ":\n",
            "\n",
            "\n",
            "Not patching Numba\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
            "  warnings.warn(str(e))\n",
            "/usr/local/lib/python3.10/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Function \"cuInit\" not found\n",
            "  warnings.warn(str(e))\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "# Core Python Libraries\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import gc\n",
        "import joblib\n",
        "from types import SimpleNamespace\n",
        "from operator import itemgetter\n",
        "from abc import abstractmethod\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Core Data manipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, ConstantInputWarning\n",
        "\n",
        "# Visualization Library\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "# Spatial Data Processing\n",
        "import spatialdata as sd  # Manage multi-modal spatial omics datasets\n",
        "import anndata as ad  # Manage annotated data matrices in memory and on disk\n",
        "import scanpy as sc  # For analyzing single-cell data, especially for dimensionality reduction and clustering.\n",
        "from skimage.measure import regionprops  # Get region properties of nucleus/cell image from masked nucleus image\n",
        "import h5py  # For handling HDF5 data files\n",
        "\n",
        "# Frameworks for ML and DL models\n",
        "import torch\n",
        "import timm  # timm: A library to load pretrained SOTA computer vision models (e.g. classification, feature extraction, ...)\n",
        "from sklearn.linear_model import Ridge  # Regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_767G4nCso"
      },
      "source": [
        "## Utilities for Saving and Reading HDF5 Files\n",
        "\n",
        "This section contains functions for efficiently saving and loading data from HDF5 files, which is a common format for storing large datasets.\n",
        "\n",
        "*   `save_hdf5` & `read_assets_from_h5`: Functions for saving and reading datasets and attributes to/from HDF5 files.\n",
        "*   `Patcher` class: Extracts image patches from a larger image using given coordinates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTzYKCIfENgW"
      },
      "source": [
        "![patcher](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/patcher.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n4N7eth0Sfss"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "#  Utilities for saving and reading HDF5 files\n",
        "####\n",
        "\n",
        "def save_hdf5(output_fpath, asset_dict, attr_dict=None, mode='a', auto_chunk=True, chunk_size=None):\n",
        "    \"\"\"\n",
        "    Save data and attributes into an HDF5 file, or initialize a new file with the given data.\n",
        "\n",
        "    Parameters:\n",
        "        output_fpath (str): Path to save the HDF5 file.\n",
        "        asset_dict (dict): Dictionary containing keys and their corresponding data (e.g., numpy arrays) to save.\n",
        "        attr_dict (dict, optional): Dictionary of attributes for each key. Format: {key: {attr_key: attr_val, ...}}.\n",
        "        mode (str): File mode ('a' for append, 'w' for write, etc.).\n",
        "        auto_chunk (bool): Whether to enable automatic chunking for HDF5 datasets.\n",
        "        chunk_size (int, optional): If auto_chunk is False, specify the chunk size for the first dimension.\n",
        "\n",
        "    Returns:\n",
        "        str: Path of the saved HDF5 file.\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(output_fpath, mode) as f:\n",
        "        for key, val in asset_dict.items():\n",
        "            data_shape = val.shape\n",
        "            # Ensure data has at least 2 dimensions\n",
        "            if len(data_shape) == 1:\n",
        "                val = np.expand_dims(val, axis=1)\n",
        "                data_shape = val.shape\n",
        "\n",
        "            if key not in f:  # if key does not exist, create a new dataset\n",
        "                data_type = val.dtype\n",
        "\n",
        "                if data_type.kind == 'U':  # Handle Unicode strings\n",
        "                    chunks = (1, 1)\n",
        "                    max_shape = (None, 1)\n",
        "                    data_type = h5py.string_dtype(encoding='utf-8')\n",
        "                else:\n",
        "                    if data_type == np.object_:\n",
        "                        data_type = h5py.string_dtype(encoding='utf-8')\n",
        "                    # Determine chunking strategy\n",
        "                    if auto_chunk:\n",
        "                        chunks = True  # let h5py decide chunk size\n",
        "                    else:\n",
        "                        chunks = (chunk_size,) + data_shape[1:]\n",
        "                    maxshape = (None,) + data_shape[1:]  # Allow unlimited size for the first dimension\n",
        "\n",
        "                try:\n",
        "                    dset = f.create_dataset(key,\n",
        "                                            shape=data_shape,\n",
        "                                            chunks=chunks,\n",
        "                                            maxshape=maxshape,\n",
        "                                            dtype=data_type)\n",
        "                    # Save attributes for the dataset\n",
        "                    if attr_dict is not None:\n",
        "                        if key in attr_dict.keys():\n",
        "                            for attr_key, attr_val in attr_dict[key].items():\n",
        "                                dset.attrs[attr_key] = attr_val\n",
        "                    # Write the data to the dataset\n",
        "                    dset[:] = val\n",
        "                except:\n",
        "                    print(f\"Error encoding {key} of dtype {data_type} into hdf5\")\n",
        "\n",
        "            else:  # Append data to an existing dataset\n",
        "                dset = f[key]\n",
        "                dset.resize(len(dset) + data_shape[0], axis=0)\n",
        "                # assert dset.dtype == val.dtype\n",
        "                dset[-data_shape[0]:] = val\n",
        "\n",
        "    return output_fpath\n",
        "\n",
        "\n",
        "def read_assets_from_h5(h5_path, keys=None, skip_attrs=False, skip_assets=False):\n",
        "    \"\"\"\n",
        "    Read data and attributes from an HDF5 file.\n",
        "\n",
        "    Parameters:\n",
        "        h5_path (str): Path to the HDF5 file.\n",
        "        keys (list, optional): List of keys to read. Reads all keys if None.\n",
        "        skip_attrs (bool): If True, skip reading attributes.\n",
        "        skip_assets (bool): If True, skip reading data assets.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A dictionary of data assets and a dictionary of attributes.\n",
        "    \"\"\"\n",
        "\n",
        "    assets = {}\n",
        "    attrs = {}\n",
        "    with h5py.File(h5_path, 'r') as f:\n",
        "        if keys is None:\n",
        "            keys = list(f.keys())\n",
        "\n",
        "        for key in keys:\n",
        "            if not skip_assets:\n",
        "                assets[key] = f[key][:]\n",
        "            if not skip_attrs and f[key].attrs is not None:\n",
        "                attrs[key] = dict(f[key].attrs)\n",
        "\n",
        "    return assets, attrs\n",
        "\n",
        "\n",
        "class Patcher:\n",
        "    def __init__(self, image, coords, patch_size_target, name=None):\n",
        "        \"\"\"\n",
        "        Initializes the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
        "\n",
        "        :param image: Input image as a numpy array (H x W x 3), the input image from which patches will be extracted.\n",
        "        :param coords: List or array of cell coordinates (centroïd) [(x1, y1), (x2, y2), ...].\n",
        "        :param patch_size_target: Target size of patches.\n",
        "        :param name: Name of the whole slide image (optional).\n",
        "        \"\"\"\n",
        "\n",
        "        self.image = image\n",
        "        self.height, self.width = image.shape[:2]\n",
        "        self.coords = coords\n",
        "        self.patch_size_target = patch_size_target\n",
        "        self.name = name\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Iterates over coordinates, yielding image patches and their coordinates.\n",
        "        \"\"\"\n",
        "\n",
        "        for x, y in self.coords:\n",
        "            # Extract patch dimension centered at (x, y)\n",
        "            x_start = max(x - self.patch_size_target // 2, 0)\n",
        "            y_start = max(y - self.patch_size_target // 2, 0)\n",
        "            x_end = min(x_start + self.patch_size_target, self.width)\n",
        "            y_end = min(y_start + self.patch_size_target, self.height)\n",
        "\n",
        "            # Ensure the patch size matches the target size, padding with zeros if necessary\n",
        "            patch = np.zeros((self.patch_size_target, self.patch_size_target, 3), dtype=np.uint8)\n",
        "            patch[:y_end - y_start, :x_end - x_start, :] = self.image[y_start:y_end, x_start:x_end, :]\n",
        "\n",
        "            yield patch, x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of patches based on the number of coordinates.\n",
        "        This is used to determine how many iterations will be done when iterating over the object.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.coords)\n",
        "\n",
        "    def save_visualization(self, path, vis_width=300, dpi=150):\n",
        "        \"\"\"\n",
        "        Save a visualization of patches overlayed on the tissue H&E image.\n",
        "        This function creates a plot where each patch's location is marked with a rectangle overlaid on the image.\n",
        "\n",
        "        :param path: File path where the visualization will be saved.\n",
        "        :param vis_width: Target width of the visualization in pixels.\n",
        "        :param dpi: Resolution of the saved visualization.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generate the tissue visualization mask\n",
        "        mask_plot = self.image\n",
        "\n",
        "        # Calculate downscale factor for visualization\n",
        "        downscale_vis = vis_width / self.width\n",
        "\n",
        "        # Create a plot\n",
        "        _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
        "        ax.imshow(mask_plot)\n",
        "\n",
        "        # Add patches\n",
        "        patch_rectangles = []\n",
        "        for x, y in self.coords:\n",
        "            x_start, y_start = x - self.patch_size_target // 2, y - self.patch_size_target // 2\n",
        "            patch_rectangles.append(Rectangle((x_start, y_start), self.patch_size_target, self.patch_size_target))\n",
        "\n",
        "        # Add rectangles to the plot\n",
        "        ax.add_collection(PatchCollection(patch_rectangles, facecolor='none', edgecolor='black', linewidth=0.3))\n",
        "\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def view_coord_points(self, vis_width=300, dpi=150):\n",
        "        \"\"\"\n",
        "        Visualizes the coordinates as small points in 2D.\n",
        "        This function generates a scatter plot of the patch coordinates on the H&E image.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate downscale factor for visualization\n",
        "        downscale_vis = vis_width / self.width\n",
        "\n",
        "        # Create a plot\n",
        "        _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
        "        plt.scatter(self.coords[:, 0], -self.coords[:, 1], s=0.2)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def to_h5(self, path, extra_assets={}):\n",
        "        \"\"\"\n",
        "        Saves the extracted patches and their associated information to an HDF5 file.\n",
        "\n",
        "        Each patch is saved as a dataset along with its coordinates and any additional assets (extra_assets).\n",
        "        The HDF5 file is structured with a dataset for the image patch ('img') and coordinates ('coords').\n",
        "\n",
        "        :param path: File path where the HDF5 file will be saved.\n",
        "        :param extra_assets: Dictionary of additional assets to save (optional). Each value in extra_assets must have the same length as the patches.\n",
        "        \"\"\"\n",
        "\n",
        "        mode_HE = 'w'  # Start with write mode for the first patch\n",
        "        i = 0\n",
        "\n",
        "        # Check that the extra_assets match the number of patches\n",
        "        if extra_assets:\n",
        "            for _, value in extra_assets.items():\n",
        "                if len(value) != len(self):\n",
        "                    raise ValueError(\"Each value in extra_assets must have the same length as the patcher object.\")\n",
        "\n",
        "        # Ensure the file has the correct extension\n",
        "        if not (path.endswith('.h5') or path.endswith('.h5ad')):\n",
        "            path = path + '.h5'\n",
        "\n",
        "        # Loop through each patch and save it to the HDF5 file (loop through __iter__ function)\n",
        "        for tile, x, y in tqdm(self):\n",
        "            assert tile.shape == (self.patch_size_target, self.patch_size_target, 3)\n",
        "\n",
        "            # Prepare the data to be saved for this patch\n",
        "            asset_dict = {\n",
        "                'img': np.expand_dims(tile, axis=0),  # Shape (1, h, w, 3)\n",
        "                'coords': np.expand_dims([x, y], axis=0)  # Shape (1, 2)\n",
        "            }\n",
        "\n",
        "            # Add any extra assets to the asset dictionary\n",
        "            extra_asset_dict = {key: np.expand_dims([value[i]], axis=0) for key, value in extra_assets.items()}\n",
        "            asset_dict = {**asset_dict, **extra_asset_dict}\n",
        "\n",
        "            # Define the attributes for the image patch\n",
        "            attr_dict = {'img': {'patch_size_target': self.patch_size_target}}\n",
        "\n",
        "            if self.name is not None:\n",
        "                attr_dict['img']['name'] = self.name\n",
        "\n",
        "            # Save the patch data to the HDF5 file\n",
        "            save_hdf5(path, asset_dict, attr_dict, mode=mode_HE, auto_chunk=False, chunk_size=1)\n",
        "            mode_HE = 'a'  # Switch to append mode after the first patch\n",
        "            i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oneR4KRvpMbS"
      },
      "source": [
        "## Preprocessing Spatial Transcriptomics Data\n",
        "\n",
        "This section contains functions for preprocessing spatial transcriptomics data, including extracting spatial coordinates, generating image patches and preparing datasets for training and testing.\n",
        "\n",
        "* **`extract_spatial_positions`**: Extracts spatial coordinates (centroids) of cells.\n",
        "* **`process_and_visualize_image`**: Extracts square image patches from H&E images and visualizes them.\n",
        "* **`preprocess_spatial_transcriptomics_data_train`**: Prepares training data by generating gene expression (Y) and image patch datasets (X).\n",
        "* **`preprocess_spatial_transcriptomics_data_test`**: Prepares test data by generating image patches (X) for selected cells.\n",
        "* **`create_cross_validation_splits`**: Creates leave-one-out cross-validation splits for model evaluation.\n",
        "\n",
        "![data_X_Y](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/data_X_Y.png)\n",
        "\n",
        "`Leave-one-out cross-validation schema:`\n",
        "![cross_validation](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/cross_validation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DRqmAKSqYtJS"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# Preprocessing spatial transcriptomics data\n",
        "##########\n",
        "\n",
        "def extract_spatial_positions(sdata, cell_id_list):\n",
        "    \"\"\"\n",
        "    Extracts spatial positions (centroids) of regions from the nucleus image where cell IDs match the provided cell list.\n",
        "\n",
        "    Need to use 'HE_nuc_original' to extract spatial coordinate of cells\n",
        "    HE_nuc_original: The nucleus segmentation mask of H&E image, in H&E native coordinate system. The cell_id in this segmentation mask matches with the nuclei by gene matrix stored in anucleus.\n",
        "    HE_nuc_original is like a binary segmentation mask 0 - 1 but replace 1 with cell_ids.\n",
        "    You can directly find the location of a cell, with cell_id, through HE_nuc_original==cell_id\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    sdata: SpatialData\n",
        "        A spatial data object containing the nucleus segmentation mask ('HE_nuc_original').\n",
        "    cell_id_list: array-like\n",
        "        A list or array of cell IDs to filter the regions.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray\n",
        "        A NumPy array of spatial coordinates (x_center, y_center) for matched regions.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Extracting spatial positions ...\")\n",
        "    # Get region properties from the nucleus image: for each cell_id get its location on HE image\n",
        "    regions = regionprops(sdata['HE_nuc_original'][0, :, :].to_numpy())\n",
        "\n",
        "    dict_spatial_positions = {}\n",
        "    # Loop through each region and extract centroid if the cell ID matches\n",
        "    for props in tqdm(regions):\n",
        "        cell_id = props.label\n",
        "        centroid = props.centroid\n",
        "        # Extract only coordinates from the provided cell_id list\n",
        "        if cell_id in cell_id_list:\n",
        "            y_center, x_center = int(centroid[0]), int(centroid[1])\n",
        "            dict_spatial_positions[cell_id] = [x_center, y_center]\n",
        "\n",
        "    # To maintain cell IDs order\n",
        "    spatial_positions = []\n",
        "    for cell_id in cell_id_list:\n",
        "        try:\n",
        "            spatial_positions.append(dict_spatial_positions[cell_id])\n",
        "        except KeyError:\n",
        "            print(f\"Warning: Cell ID {cell_id} not found in the segmentation mask.\")\n",
        "            spatial_positions.append([1000, 1000])\n",
        "\n",
        "    return np.array(spatial_positions)\n",
        "\n",
        "\n",
        "def process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
        "                                show_extracted_images=False, vis_width=1000, overwrite=False):\n",
        "    \"\"\"\n",
        "    Load and process the spatial image data, creates patches, saves them in an HDF5 file,\n",
        "    and visualizes the extracted images and spatial coordinates.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    sdata: SpatialData\n",
        "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
        "    patch_save_dir: str\n",
        "        Directory where the resulting HDF5 file and visualizations will be saved.\n",
        "    name_data: str\n",
        "        Name used for saving the dataset.\n",
        "    coords_center: array-like\n",
        "        Coordinates of the regions to be patched (centroids of cell regions).\n",
        "    target_patch_size: int\n",
        "        Size of the patches to extract from the image.\n",
        "    barcodes: array-like\n",
        "        Barcodes associated with patches.\n",
        "    show_extracted_images: bool, optional (default=False)\n",
        "        If True, will show extracted images during the visualization phase.\n",
        "    vis_width: int, optional (default=1000)\n",
        "        Width of the visualization images.\n",
        "    \"\"\"\n",
        "    # Path for the .h5 image dataset\n",
        "    h5_path = os.path.join(patch_save_dir, name_data + '.h5')\n",
        "    if (os.path.exists(h5_path)) and (not overwrite):\n",
        "        print(\"H5 file already exists\")\n",
        "        return\n",
        "\n",
        "    # Load the image and transpose it to the correct format\n",
        "    print(\"Loading imgs ...\")\n",
        "    intensity_image = np.transpose(sdata['HE_original'].to_numpy(), (1, 2, 0))\n",
        "\n",
        "    print(\"Patching: create image dataset (X) ...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
        "    patcher = Patcher(\n",
        "        image=intensity_image,\n",
        "        coords=coords_center,\n",
        "        patch_size_target=target_patch_size\n",
        "    )\n",
        "\n",
        "    # Build and Save patches to an HDF5 file\n",
        "    patcher.to_h5(h5_path, extra_assets={'barcode': barcodes})\n",
        "\n",
        "    # Visualization\n",
        "    print(\"Visualization\")\n",
        "    if show_extracted_images:\n",
        "        print(\"Extracted Images (high time and memory consumption...)\")\n",
        "        patcher.save_visualization(os.path.join(patch_save_dir, name_data + '_viz.png'), vis_width=vis_width)\n",
        "\n",
        "    print(\"Spatial coordinates\")\n",
        "    patcher.view_coord_points(vis_width=vis_width)\n",
        "\n",
        "    # Display some example images from the created dataset\n",
        "    print(\"Examples from the created .h5 dataset\")\n",
        "    assets, _ = read_assets_from_h5(h5_path)\n",
        "\n",
        "    n_images = 3\n",
        "    fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
        "    for i in range(n_images):\n",
        "        axes[i].imshow(assets[\"img\"][i])\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Delete variables that are no longer used\n",
        "    del intensity_image, patcher, assets\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def preprocess_spatial_transcriptomics_data_train(list_ST_name_data, data_directory_path, dir_processed_dataset, size_subset=None, target_patch_size=32, vis_width=1000, show_extracted_images=False):\n",
        "    \"\"\"\n",
        "    Train step: Preprocesses spatial transcriptomics data by performing the following steps for each ST:\n",
        "    1. Samples the dataset and extract spatial coordinates of cells.\n",
        "    2. Extract gene expression data (Y) and save it as `.h5ad` files into directory 'adata'.\n",
        "    4. Generates and saves patches of images centered on spatial coordinates to HDF5 files (X) into directory 'patches'.\n",
        "    5. Saves the list of genes to a JSON file into direcotry 'splits'.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    list_ST_name_data: list\n",
        "        List of spatial transcriptomics data names.\n",
        "    data_directory_path: str\n",
        "        Path to the directory containing the input data in `.zarr` format.\n",
        "    dir_processed_dataset: str\n",
        "        Path to the directory where processed datasets and outputs will be saved.\n",
        "    size_subset: int, optional\n",
        "        ST data sample size. If None, no sampling.\n",
        "    target_patch_size: int, optional\n",
        "        Target size of image patches to extract.\n",
        "    vis_width: int, optional\n",
        "        Width of the visualization output for spatial and image patches.\n",
        "    show_extracted_images: bool\n",
        "    \"\"\"\n",
        "\n",
        "    # Creates directories for saving patches (X) ('patches'), processed AnnData objects (Y) ('adata'), and train/test dataset splits ('splits').\n",
        "    patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
        "    adata_save_dir = os.path.join(dir_processed_dataset, \"adata\")\n",
        "    splits_save_dir = os.path.join(dir_processed_dataset, \"splits\")\n",
        "    os.makedirs(patch_save_dir, exist_ok=True)\n",
        "    os.makedirs(adata_save_dir, exist_ok=True)\n",
        "    os.makedirs(splits_save_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\\n\")\n",
        "\n",
        "    # Loop through each dataset name\n",
        "    for count, name_data in enumerate(list_ST_name_data):\n",
        "        print(f\"\\nDATA ({count+1}/{len(list_ST_name_data)}): {name_data}\\n\")\n",
        "\n",
        "        # Load the spatial transcriptomics data from the .zarr format\n",
        "        sdata = sd.read_zarr(os.path.join(data_directory_path, f\"{name_data}.zarr\"))\n",
        "\n",
        "        # Extract the list of gene names\n",
        "        gene_name_list = sdata['anucleus'].var['gene_symbols'].values\n",
        "\n",
        "        # Sample the dataset if a subset size is specified\n",
        "        if size_subset is not None:\n",
        "            print(\"Sampling the dataset ...\")\n",
        "            rows_to_keep = list(sdata['anucleus'].obs.sample(n=min(size_subset, len(sdata['anucleus'].obs)), random_state=42).index)\n",
        "        else:\n",
        "            size_subset = len(sdata['anucleus'].obs)\n",
        "            rows_to_keep = list(sdata['anucleus'].obs.sample(n=size_subset, random_state=42).index)\n",
        "\n",
        "        # Extract spatial positions for 'train' cells\n",
        "        cell_id_train = sdata['anucleus'].obs[\"cell_id\"].values\n",
        "        new_spatial_coord = extract_spatial_positions(sdata, cell_id_train)\n",
        "        # Store new spatial coordinates into sdata\n",
        "        sdata['anucleus'].obsm['spatial'] = new_spatial_coord\n",
        "\n",
        "        # Create the gene expression dataset (Y)\n",
        "        print(\"Create gene expression dataset (Y) ...\")\n",
        "        y_subtracted = sdata['anucleus'][rows_to_keep].copy()\n",
        "        # Trick to set all index to same length to avoid problems when saving to h5\n",
        "        y_subtracted.obs.index = ['x' + str(i).zfill(6) for i in y_subtracted.obs.index]\n",
        "\n",
        "        # Save the gene expression data to an H5AD file\n",
        "        y_subtracted.write(os.path.join(adata_save_dir, f'{name_data}.h5ad'))\n",
        "\n",
        "        for index in y_subtracted.obs.index:\n",
        "            if len(index) != len(y_subtracted.obs.index[0]):\n",
        "                warnings.warn(\"indices of y_subtracted.obs should all have the same length to avoid problems when saving to h5\", UserWarning)\n",
        "\n",
        "        # Extract spatial coordinates and barcodes (cell IDs) for the patches\n",
        "        coords_center = y_subtracted.obsm['spatial']\n",
        "        barcodes = np.array(y_subtracted.obs.index)\n",
        "\n",
        "        # Generate and visualize image patches centered around spatial coordinates ({name_data}.h5 file in directory os.path.join(dir_processed_dataset, \"patches\"))\n",
        "        process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
        "                                    show_extracted_images=False, vis_width=1000)\n",
        "\n",
        "        # Delete variables that are no longer used\n",
        "        del sdata, y_subtracted\n",
        "        gc.collect()\n",
        "\n",
        "    # Save the gene list to a JSON file\n",
        "    gene_path = os.path.join(dir_processed_dataset, 'var_genes.json')\n",
        "    print(f\"Save gene list in {gene_path}\")\n",
        "    data = {\n",
        "        \"genes\": list(gene_name_list)\n",
        "    }\n",
        "    print(\"Total number of genes:\", len(data[\"genes\"]))\n",
        "\n",
        "    with open(gene_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(\"\\nPreprocess dataset DONE:\", \" - \".join(list_ST_name_data), \"\\n\")\n",
        "\n",
        "\n",
        "def preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_id_list, dir_processed_dataset, target_patch_size=32, vis_width=1000, show_extracted_images=False):\n",
        "    \"\"\"\n",
        "    Test step: Preprocesses spatial transcriptomics data by performing the following steps for the selected ST data:\n",
        "    1. Extract spatial coordinates of the selected cells.\n",
        "    2. Generates and saves patches of images centered on spatial coordinates to HDF5 files (X) into directory 'patches'.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    name_data: str\n",
        "        Name used for saving the dataset.\n",
        "    sdata: SpatialData\n",
        "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
        "    cell_id_list : array-like\n",
        "        A list or array of cell IDs to filter the regions.\n",
        "    dir_processed_dataset: str\n",
        "        Path to the directory where processed datasets and outputs will be saved.\n",
        "    target_patch_size: int, optional\n",
        "        Target size of image patches to extract.\n",
        "    vis_width: int, optional\n",
        "        Width of the visualization output for spatial and image patches.\n",
        "    show_extracted_images: bool\n",
        "    \"\"\"\n",
        "\n",
        "    # Creates directories for saving patches ('patches')\n",
        "    patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
        "    os.makedirs(patch_save_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\\n\")\n",
        "\n",
        "    # Extract spatial positions for selected cells\n",
        "    new_spatial_coord = extract_spatial_positions(sdata, cell_id_list)\n",
        "\n",
        "    # Spatial coordinates and barcodes (cell IDs) for the patches\n",
        "    coords_center = new_spatial_coord\n",
        "    barcodes = np.array(['x' + str(i).zfill(6) for i in list(cell_id_list)])  # Trick to set all index to same length to avoid problems when saving to h5\n",
        "\n",
        "    # Generate and visualize image patches centered around spatial coordinates ({name_data}.h5 file in directory os.path.join(dir_processed_dataset, \"patches\"))\n",
        "    process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
        "                                show_extracted_images=False, vis_width=1000)\n",
        "\n",
        "    print(\"\\nPreprocess dataset DONE\\n\")\n",
        "\n",
        "\n",
        "def create_cross_validation_splits(dir_processed_dataset, n_fold=None):\n",
        "    \"\"\"\n",
        "    Creates cross-validation splits (leave-one-out cv) for spatial transcriptomics data by splitting\n",
        "    samples into training and testing sets and saving them as CSV files.\n",
        "\n",
        "    Example for samples [\"UC1_NI\", \"UC1_I\", \"UC6_NI\"]:\n",
        "      FOLD 0: TRAIN: [\"UC1_NI\", \"UC1_I\"] TEST: [\"UC6_NI\"]\n",
        "      FOLD 1: TRAIN: [\"UC1_NI\", \"UC6_NI\"] TEST: [\"UC1_I\"]\n",
        "      FOLD 2: TRAIN: [\"UC6_NI\", \"UC1_I\"] TEST: [\"UC1_NI\"]\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dir_processed_dataset : str\n",
        "        Path to the directory where processed datasets are saved.\n",
        "    n_fold : int, optional\n",
        "        Number of folds for cross-validation (leave-one-out cv). If None, defaults to number of ST files.\n",
        "    \"\"\"\n",
        "\n",
        "    patches_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
        "    splits_dir = os.path.join(dir_processed_dataset, \"splits\")\n",
        "    os.makedirs(splits_dir, exist_ok=True)\n",
        "\n",
        "    # List all files in the patches directory (these represent individual samples)\n",
        "    patch_files = os.listdir(patches_dir)\n",
        "\n",
        "    # Prepare a list to store information about the samples (patches and gene expression data path)\n",
        "    all_ST = []\n",
        "    # Extra paths by iterating over patch files\n",
        "    for patch_file in patch_files:\n",
        "        if patch_file.endswith('.h5'):\n",
        "            # Extract sample ID from patch file name\n",
        "            sample_id = patch_file.split('.')[0]\n",
        "            # Corresponding gene expression data file (should be in 'adata' directory)\n",
        "            expr_file = os.path.join(\"adata\", f\"{sample_id}.h5ad\")\n",
        "            all_ST.append({\n",
        "                \"sample_id\": sample_id,\n",
        "                \"patches_path\": os.path.join(\"patches\", patch_file),\n",
        "                \"expr_path\": expr_file\n",
        "            })\n",
        "\n",
        "    df_all_ST = pd.DataFrame(all_ST)\n",
        "\n",
        "    # If n_fold is not specified, default to using the number of available samples\n",
        "    if n_fold is None:\n",
        "        n_fold = len(df_all_ST)\n",
        "    # Ensure that the number of folds does not exceed the number of available samples\n",
        "    n_fold = min(n_fold, len(df_all_ST))\n",
        "\n",
        "    print(\"\\n -- CREATE CROSS-VALIDATION SPLITS --------------------------------------------\\n\")\n",
        "\n",
        "    # Generate cross-validation splits (leave-one-out CV)\n",
        "    for i in range(n_fold):\n",
        "        # Select the current sample as the test set (leave-one-out)\n",
        "        test_df = df_all_ST.iloc[[i]]\n",
        "        # Use the remaining samples as the training set\n",
        "        train_df = df_all_ST.drop(i)\n",
        "\n",
        "        print(f\"Index {i}:\")\n",
        "        print(\"Train DataFrame:\")\n",
        "        print(train_df)\n",
        "        print(\"Test DataFrame:\")\n",
        "        print(test_df)\n",
        "\n",
        "        # Save the train and test DataFrames as CSV files in the splits directory\n",
        "        train_filename = f\"train_{i}.csv\"\n",
        "        test_filename = f\"test_{i}.csv\"\n",
        "        train_df.to_csv(os.path.join(splits_dir, train_filename), index=False)\n",
        "        test_df.to_csv(os.path.join(splits_dir, test_filename), index=False)\n",
        "        print(f\"Saved {train_filename} and {test_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koaqf4aXwOV0"
      },
      "source": [
        "## Utility functions\n",
        "\n",
        "These utility functions assist with gene expression data processing and model results handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iyslxSHLrV_6"
      },
      "outputs": [],
      "source": [
        "###############\n",
        "# utils fct\n",
        "###############\n",
        "\n",
        "def log1p_normalization(arr):\n",
        "    \"\"\"  Apply log1p normalization to the given array \"\"\"\n",
        "\n",
        "    scale_factor = 100\n",
        "    return np.log1p((arr / np.sum(arr, axis=1, keepdims=True)) * scale_factor)\n",
        "\n",
        "\n",
        "def normalize_adata(adata: sc.AnnData) -> sc.AnnData:\n",
        "    \"\"\"\n",
        "    Normalize and apply log1p transformation to the expression matrix of an AnnData object.\n",
        "    (The function normalizes the gene expression by row)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    adata : sc.AnnData\n",
        "        AnnData object containing gene expression data.\n",
        "    \"\"\"\n",
        "\n",
        "    filtered_adata = adata.copy()\n",
        "    filtered_adata.X = filtered_adata.X.astype(np.float64)\n",
        "    filtered_adata.X = log1p_normalization(filtered_adata.X)\n",
        "\n",
        "    return filtered_adata\n",
        "\n",
        "\n",
        "def load_adata(expr_path, genes=None, barcodes=None, normalize=False):\n",
        "    \"\"\"\n",
        "    Load AnnData object from a given path\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    expr_path : str\n",
        "        Path to the .h5ad file containing the AnnData object.\n",
        "    genes : list, optional\n",
        "        List of genes to retain. If None, all genes are kept.\n",
        "    barcodes : list, optional\n",
        "        List of barcodes (cells) to retain. If None, all cells are kept.\n",
        "    normalize : bool, optional\n",
        "        Whether to apply normalization (log1p normalization) to the data.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Gene expression data as a DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    adata = sc.read_h5ad(expr_path)\n",
        "    if barcodes is not None:\n",
        "        adata = adata[barcodes]\n",
        "    if genes is not None:\n",
        "        adata = adata[:, genes]\n",
        "    if normalize:\n",
        "        adata = normalize_adata(adata)\n",
        "    return adata.to_df()\n",
        "\n",
        "\n",
        "def merge_dict(main_dict, new_dict, value_fn=None):\n",
        "    \"\"\"\n",
        "    Merge new_dict into main_dict. If a key exists in both dicts, the values are appended.\n",
        "    Else, the key-value pair is added.\n",
        "    Expects value to be an array or list - if not, it is converted to a list.\n",
        "    If value_fn is not None, it is applied to each item in each value in new_dict before merging.\n",
        "    Args:\n",
        "        main_dict: main dict\n",
        "        new_dict: new dict\n",
        "        value_fn: function to apply to each item in each value in new_dict before merging\n",
        "    \"\"\"\n",
        "\n",
        "    if value_fn is None:\n",
        "        def value_fn(x): return x\n",
        "\n",
        "    for key, value in new_dict.items():\n",
        "        if not isinstance(value, list):\n",
        "            value = [value]\n",
        "        value = [value_fn(v) for v in value]\n",
        "        if key in main_dict:\n",
        "            main_dict[key] = main_dict[key] + value\n",
        "        else:\n",
        "            main_dict[key] = value\n",
        "\n",
        "    return main_dict\n",
        "\n",
        "\n",
        "def post_collate_fn(batch):\n",
        "    \"\"\" Post collate function to clean up batch \"\"\"\n",
        "\n",
        "    if batch[\"imgs\"].dim() == 5:\n",
        "        assert batch[\"imgs\"].size(0) == 1\n",
        "        batch[\"imgs\"] = batch[\"imgs\"].squeeze(0)\n",
        "\n",
        "    if batch[\"coords\"].dim() == 3:\n",
        "        assert batch[\"coords\"].size(0) == 1\n",
        "        batch[\"coords\"] = batch[\"coords\"].squeeze(0)\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def merge_fold_results(arr):\n",
        "    \"\"\" Merges results from multiple cross-validation folds, aggregating Pearson correlation across all folds. \"\"\"\n",
        "    aggr_dict = {}\n",
        "    for dict in arr:\n",
        "        for item in dict['pearson_corrs']:\n",
        "            gene_name = item['name']\n",
        "            correlation = item['pearson_corr']\n",
        "            aggr_dict[gene_name] = aggr_dict.get(gene_name, []) + [correlation]\n",
        "\n",
        "    aggr_results = []\n",
        "    all_corrs = []\n",
        "    for key, value in aggr_dict.items():\n",
        "        aggr_results.append({\n",
        "            \"name\": key,\n",
        "            \"pearson_corrs\": value,\n",
        "            \"mean\": np.mean(value),\n",
        "            \"std\": np.std(value)\n",
        "        })\n",
        "        all_corrs += value\n",
        "\n",
        "    mean_per_split = [d['pearson_mean'] for d in arr]\n",
        "\n",
        "    return {\n",
        "        \"pearson_corrs\": aggr_results,\n",
        "        \"pearson_mean\": np.mean(mean_per_split),\n",
        "        \"pearson_std\": np.std(mean_per_split),\n",
        "        \"mean_per_split\": mean_per_split\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tBRiWNowz6b"
      },
      "source": [
        "## Encoder functions\n",
        "\n",
        "Provide the structure for building and utilizing pre-trained models specifically designed for extracting features from images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MBn6-RVJ4THe"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# Encoder functions\n",
        "##########\n",
        "\n",
        "class InferenceEncoder(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Abstract base class for building inference encoders.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    weights_path : str or None\n",
        "        Path to the model weights (optional).\n",
        "    model : torch.nn.Module\n",
        "        The model architecture.\n",
        "    eval_transforms : callable\n",
        "        Evaluation transformations applied to the input images.\n",
        "    precision : torch.dtype\n",
        "        The data type of the model's parameters and inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights_path=None, **build_kwargs):\n",
        "        super(InferenceEncoder, self).__init__()\n",
        "\n",
        "        self.weights_path = weights_path\n",
        "        self.model, self.eval_transforms, self.precision = self._build(weights_path, **build_kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.model(x)\n",
        "        return z\n",
        "\n",
        "    @abstractmethod\n",
        "    def _build(self, **build_kwargs):\n",
        "        pass\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "def get_eval_transforms(mean, std):\n",
        "    \"\"\"\n",
        "    Creates the evaluation transformations for preprocessing images. This includes\n",
        "    converting the images to tensor format and normalizing them with given mean and std.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    mean : list\n",
        "        The mean values used for normalization.\n",
        "    std : list\n",
        "        The standard deviation values used for normalization.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    transforms.Compose\n",
        "        A composed transformation function that applies the transformations in sequence.\n",
        "    \"\"\"\n",
        "    trsforms = []\n",
        "\n",
        "    # Convert image to tensor\n",
        "    trsforms.append(lambda img: TF.to_tensor(img))\n",
        "\n",
        "    if mean is not None and std is not None:\n",
        "        # Normalize the image\n",
        "        trsforms.append(lambda img: TF.normalize(img, mean, std))\n",
        "\n",
        "    return transforms.Compose(trsforms)\n",
        "\n",
        "class ResNet50InferenceEncoder(InferenceEncoder):\n",
        "    \"\"\"\n",
        "    A specific implementation of the InferenceEncoder class for ResNet50.\n",
        "    This encoder is used to extract features from images using a pretrained ResNet50 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def _build(\n",
        "        self,\n",
        "        weights_root=\"resnet50.tv_in1k\",\n",
        "        timm_kwargs={\"features_only\": True, \"out_indices\": [3], \"num_classes\": 0},\n",
        "        pool=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Build the ResNet50 model and load its weights. It supports both pretrained models\n",
        "        from the internet and pretrained models from a given weights path (offline).\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        weights_root : str\n",
        "            Path to pretrained model weights. Defaults to \"resnet50.tv_in1k\" (if online).\n",
        "        timm_kwargs : dict\n",
        "            Additional arguments for creating the ResNet50 model via the timm library.\n",
        "        pool : bool\n",
        "            Whether to apply adaptive average pooling to the output of the model. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            A tuple containing the ResNet50 model, the evaluation transformations, and the precision type.\n",
        "        \"\"\"\n",
        "\n",
        "        if weights_root == \"resnet50.tv_in1k\":\n",
        "            pretrained = True\n",
        "            print(\"Load pretrained Resnet50 from internet\")\n",
        "        else:\n",
        "            pretrained = False\n",
        "            print(f\"Load pretrained Resnet50 offline from weights path: {weights_root}\")\n",
        "\n",
        "        # Build the model using the timm library\n",
        "        model = timm.create_model(\"resnet50.tv_in1k\", pretrained=pretrained, **timm_kwargs)\n",
        "\n",
        "        # If not using a pretrained model, load weights from the specified path\n",
        "        if not pretrained and os.path.exists(weights_root):\n",
        "            # Load the weights\n",
        "            checkpoint = torch.load(weights_root, map_location='cpu', weights_only=True)  # or 'cuda' if using GPU\n",
        "\n",
        "            # Remove the classifier layers from the checkpoint\n",
        "            model_state_dict = model.state_dict()\n",
        "            checkpoint = {k: v for k, v in checkpoint.items() if k in model_state_dict}\n",
        "\n",
        "            # Load the weights into the model\n",
        "            model.load_state_dict(checkpoint, strict=False)\n",
        "        elif not pretrained:\n",
        "            # Issue a warning if the weights file is missing\n",
        "            print(f\"\\n!!! WARNING: The specified weights file '{weights_root}' does not exist. The model will be initialized with random weights.\\n\")\n",
        "\n",
        "        imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "        eval_transform = get_eval_transforms(imagenet_mean, imagenet_std)\n",
        "        precision = torch.float32\n",
        "        if pool:\n",
        "            self.pool = torch.nn.AdaptiveAvgPool2d(1)\n",
        "        else:\n",
        "            self.pool = None\n",
        "\n",
        "        return model, eval_transform, precision\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.forward_features(x)\n",
        "        if self.pool:\n",
        "            out = self.pool(out).squeeze(-1).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        out = self.model(x)\n",
        "        if isinstance(out, list):\n",
        "            assert len(out) == 1\n",
        "            out = out[0]\n",
        "        return out\n",
        "\n",
        "def inf_encoder_factory(enc_name):\n",
        "    \"\"\"\n",
        "    Factory function to instantiate an encoder based on the specified name.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    enc_name : str\n",
        "        The name of the encoder model to instantiate (e.g., 'resnet50').\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    class\n",
        "        The encoder class corresponding to the specified encoder name.\n",
        "    \"\"\"\n",
        "\n",
        "    if enc_name == 'resnet50':\n",
        "        return ResNet50InferenceEncoder\n",
        "\n",
        "    raise ValueError(f\"Unknown encoder name {enc_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_gewG-TqZL_"
      },
      "source": [
        "## Torch Dataset & Embeddings\n",
        "\n",
        "This section defines a custom torch dataset for handling spatial transcriptomics data and generating embeddings for image patches.\n",
        "\n",
        "- **`H5Dataset`**: A torch `Dataset` class for loading spatial transcriptomics data from an HDF5 file. It loads image patches, their associated barcodes/cells and coordinates in batches to efficiently handle large datasets.\n",
        "- **`generate_embeddings`**: A utility for generating embeddings from images  and saving them to an HDF5 file. It handles creating a `DataLoader`, running the model in evaluation mode and saving embeddings to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g3pPQPc6EKEq"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "# Torch Dataset & Embeddings\n",
        "#########\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class H5Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset to read ST + H&E from an HDF5 (.h5) file\n",
        "    The dataset loads images and their associated barcodes/cells and coordinates in chunks for efficient data handling.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    h5_path : str\n",
        "        Path to the HDF5 file containing the images, barcodes, and coordinates.\n",
        "    img_transform : callable, optional\n",
        "        A transformation function to apply to the images. Defaults to None.\n",
        "    chunk_size : int, optional\n",
        "        Number of items to load per batch. Defaults to 1000.\n",
        "    n_chunks : int\n",
        "        The total number of chunks, calculated based on the size of the 'barcode' array.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, h5_path, img_transform=None, chunk_size=1000):\n",
        "        self.h5_path = h5_path\n",
        "        self.img_transform = img_transform\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            self.n_chunks = int(np.ceil(len(f['barcode']) / chunk_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_chunks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a batch of data (images, barcodes, and coordinates) from the HDF5 file.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        idx : int\n",
        "            The index of the chunk to fetch.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            A dictionary containing the images, barcodes, and coordinates for the specified chunk.\n",
        "        \"\"\"\n",
        "\n",
        "        start_idx = idx * self.chunk_size\n",
        "        end_idx = (idx + 1) * self.chunk_size\n",
        "        # Open the HDF5 file and load the specific chunk of data\n",
        "        with h5py.File(self.h5_path, 'r') as f:\n",
        "            imgs = f['img'][start_idx:end_idx]\n",
        "            barcodes = f['barcode'][start_idx:end_idx].flatten().tolist()\n",
        "            coords = f['coords'][start_idx:end_idx]\n",
        "\n",
        "        # Apply image transformations if any (e.g. to Torch and normalization)\n",
        "        if self.img_transform:\n",
        "            imgs = torch.stack([self.img_transform(img) for img in imgs])\n",
        "\n",
        "        return {'imgs': imgs, 'barcodes': barcodes, 'coords': coords}\n",
        "\n",
        "\n",
        "def embed_tiles(dataloader, model: torch.nn.Module, embedding_save_path: str, device: str, precision):\n",
        "    \"\"\"\n",
        "    Extracts embeddings from image tiles using the specified model and saves them to an HDF5 file.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataloader : torch.utils.data.DataLoader\n",
        "        DataLoader providing the batches of image tiles.\n",
        "    model : torch.nn.Module\n",
        "        The model used to generate embeddings from the tiles.\n",
        "    embedding_save_path : str\n",
        "        Path where the generated embeddings will be saved.\n",
        "    device : str\n",
        "        The device to run the model on (e.g., 'cuda' or 'cpu').\n",
        "    precision : torch.dtype\n",
        "        The precision (data type) to use for inference (e.g., float16 for mixed precision).\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    # Iterate over the batches in the DataLoader\n",
        "    for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        batch = post_collate_fn(batch)\n",
        "        imgs = batch['imgs'].to(device).float()\n",
        "        # Apply model on images\n",
        "        with torch.inference_mode():\n",
        "            if torch.cuda.is_available():  # Use mixed precision only if CUDA is available\n",
        "                with torch.amp.autocast('cuda', dtype=precision):\n",
        "                    embeddings = model(imgs)\n",
        "            else:  # No mixed precision on CPU\n",
        "                embeddings = model(imgs)\n",
        "\n",
        "        # Set mode to 'w' for the first batch, 'a' for appending subsequent batches\n",
        "        mode = 'w' if batch_idx == 0 else 'a'\n",
        "\n",
        "        # Create a dictionary with embeddings and other relevant data to save\n",
        "        asset_dict = {'embeddings': embeddings.cpu().numpy()}\n",
        "        asset_dict.update({key: np.array(val) for key, val in batch.items() if key != 'imgs'})\n",
        "\n",
        "        # Save the embeddings to the HDF5 file\n",
        "        save_hdf5(embedding_save_path, asset_dict=asset_dict, mode=mode)\n",
        "\n",
        "    return embedding_save_path\n",
        "\n",
        "\n",
        "def generate_embeddings(embed_path, encoder, device, tile_h5_path, batch_size, num_workers, overwrite=False):\n",
        "    \"\"\"\n",
        "    Generate embeddings for images and save to a specified path.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    embed_path : str\n",
        "        Path to save the embeddings.\n",
        "    encoder : torch.nn.Module\n",
        "        The encoder model for generating embeddings.\n",
        "    device : torch.device\n",
        "        Device to use for computation (e.g., 'cuda' or 'cpu').\n",
        "    tile_h5_path : str\n",
        "        Path to the HDF5 file containing images.\n",
        "    batch_size : int\n",
        "        Batch size for the DataLoader.\n",
        "    num_workers : int\n",
        "        Number of worker threads for data loading.\n",
        "    overwrite : bool, optional\n",
        "        If True, overwrite existing embeddings. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    # If the embeddings file doesn't exist or overwrite is True, proceed to generate embeddings\n",
        "    if not os.path.isfile(embed_path) or overwrite:\n",
        "        print(f\"Generating embeddings for {embed_path} ...\")\n",
        "\n",
        "        # Set encoder to evaluation mode and move it to the device\n",
        "        encoder.eval()\n",
        "        encoder.to(device)\n",
        "\n",
        "        # Create dataset and dataloader for tiles\n",
        "        tile_dataset = H5Dataset(tile_h5_path, chunk_size=batch_size, img_transform=encoder.eval_transforms)\n",
        "        tile_dataloader = torch.utils.data.DataLoader(\n",
        "            tile_dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        # Generate and save embeddings\n",
        "        embed_tiles(tile_dataloader, encoder, embed_path, device, encoder.precision)\n",
        "    else:\n",
        "        print(f\"Skipping embedding {os.path.basename(embed_path)} as it already exists\")\n",
        "\n",
        "\n",
        "def fine_tuning(embed_path, encoder, device, tile_h5_path, batch_size, num_workers, overwrite=False):\n",
        "    \"\"\"\n",
        "    Generate embeddings for images and save to a specified path.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    embed_path : str\n",
        "        Path to save the embeddings.\n",
        "    encoder : torch.nn.Module\n",
        "        The encoder model for generating embeddings.\n",
        "    device : torch.device\n",
        "        Device to use for computation (e.g., 'cuda' or 'cpu').\n",
        "    tile_h5_path : str\n",
        "        Path to the HDF5 file containing images.\n",
        "    batch_size : int\n",
        "        Batch size for the DataLoader.\n",
        "    num_workers : int\n",
        "        Number of worker threads for data loading.\n",
        "    overwrite : bool, optional\n",
        "        If True, overwrite existing embeddings. Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    # # If the embeddings file doesn't exist or overwrite is True, proceed to generate embeddings\n",
        "    # if not os.path.isfile(embed_path) or overwrite:\n",
        "    #     print(f\"Generating embeddings for {embed_path} ...\")\n",
        "\n",
        "        # Set encoder to evaluation mode and move it to the device\n",
        "    encoder.eval()\n",
        "    encoder.to(device)\n",
        "\n",
        "    # Create dataset and dataloader for tiles\n",
        "    tile_dataset = H5Dataset(tile_h5_path, chunk_size=batch_size, img_transform=encoder.eval_transforms)\n",
        "    tile_dataloader = torch.utils.data.DataLoader(\n",
        "        tile_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m95S5XlMq6gL"
      },
      "source": [
        "## Regression Functions\n",
        "\n",
        "This section defines functions for training a regression model and evaluating its performance.\n",
        "\n",
        "* **`compute_metrics`**: Computes evaluation metrics, including L2 error, R² score and Pearson correlation for each target/gene.\n",
        "* **`train_test_reg`**: Trains a regression model (default: Ridge regression) using the provided embedding vector train data and evaluates it on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_imPcy_bG89u"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# Regression functions\n",
        "##########\n",
        "\n",
        "def compute_metrics(y_test, preds_all, genes=None):\n",
        "    \"\"\"\n",
        "    Computes metrics L2 errors R2 scores and Pearson correlations for each target/gene.\n",
        "\n",
        "    :param y_test: Ground truth values (numpy array of shape [n_samples, n_targets]).\n",
        "    :param preds_all: Predictions for all targets (numpy array of shape [n_samples, n_targets]).\n",
        "    :param genes: Optional list of gene names corresponding to targets.\n",
        "    :return: A dictionary containing metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    errors = []\n",
        "    r2_scores = []\n",
        "    pearson_corrs = []\n",
        "    pearson_genes = []\n",
        "\n",
        "    for i, target in enumerate(range(y_test.shape[1])):\n",
        "        preds = preds_all[:, target]\n",
        "        target_vals = y_test[:, target]\n",
        "\n",
        "        # Compute L2 error\n",
        "        l2_error = float(np.mean((preds - target_vals) ** 2))\n",
        "\n",
        "        # Compute R2 score\n",
        "        total_variance = np.sum((target_vals - np.mean(target_vals)) ** 2)\n",
        "        if total_variance == 0:\n",
        "            r2_score = 0.0\n",
        "        else:\n",
        "            r2_score = float(1 - np.sum((target_vals - preds) ** 2) / total_variance)\n",
        "\n",
        "        # Compute Pearson correlation\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"error\", category=ConstantInputWarning)\n",
        "            try:\n",
        "                pearson_corr, _ = pearsonr(target_vals, preds)\n",
        "                pearson_corr = pearson_corr if not np.isnan(pearson_corr) else 0.0\n",
        "            except ConstantInputWarning:\n",
        "                pearson_corr = 0.0\n",
        "\n",
        "        errors.append(l2_error)\n",
        "        r2_scores.append(r2_score)\n",
        "        pearson_corrs.append(pearson_corr)\n",
        "\n",
        "        # Record gene-specific Pearson correlation\n",
        "        if genes is not None:\n",
        "            pearson_genes.append({\n",
        "                'name': genes[i],\n",
        "                'pearson_corr': pearson_corr\n",
        "            })\n",
        "\n",
        "    # Compile results\n",
        "    results = {\n",
        "        'pearson_mean': float(np.mean(pearson_corrs)),\n",
        "        'l2_errors_mean': float(np.mean(errors)),\n",
        "        'r2_scores_mean': float(np.mean(r2_scores)),\n",
        "        'l2_errors': list(errors),\n",
        "        'r2_scores': list(r2_scores),\n",
        "        'pearson_corrs': pearson_genes if genes is not None else list(pearson_corrs),\n",
        "        'pearson_std': float(np.std(pearson_corrs)),\n",
        "        'l2_error_q1': float(np.percentile(errors, 25)),\n",
        "        'l2_error_q2': float(np.median(errors)),\n",
        "        'l2_error_q3': float(np.percentile(errors, 75)),\n",
        "        'r2_score_q1': float(np.percentile(r2_scores, 25)),\n",
        "        'r2_score_q2': float(np.median(r2_scores)),\n",
        "        'r2_score_q3': float(np.percentile(r2_scores, 75)),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_test_reg(X_train, X_test, y_train, y_test,\n",
        "                   max_iter=1000, random_state=0, genes=None, alpha=None, method='ridge'):\n",
        "    \"\"\" Train a regression model and evaluate its performance on test data \"\"\"\n",
        "\n",
        "    if method == 'ridge':\n",
        "        # If alpha is not provided, compute it based on the input dimensions\n",
        "        alpha = 100 / (X_train.shape[1] * y_train.shape[1])\n",
        "\n",
        "        print(f\"Ridge: using alpha: {alpha}\")\n",
        "        # Initialize Ridge regression model\n",
        "        reg = Ridge(solver='lsqr',\n",
        "                    alpha=alpha,\n",
        "                    random_state=random_state,\n",
        "                    fit_intercept=False,\n",
        "                    max_iter=max_iter)\n",
        "        # Fit the model on the training data\n",
        "        reg.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the test data\n",
        "        preds_all = reg.predict(X_test)\n",
        "\n",
        "    # You can instantiate other regression models...\n",
        "\n",
        "    # Compute the evaluation metrics using the test data and predictions\n",
        "    results = compute_metrics(y_test, preds_all, genes=genes)\n",
        "\n",
        "    return reg, results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---kKu3RrfH4"
      },
      "source": [
        "## Training Functions\n",
        "\n",
        "This section defines functions for training models and saving results.\n",
        "\n",
        "- **`predict_single_split`**: Performs predictions for a single train-test split. It handles the embedding generation, loading of gene expression data, model training and saving the results. The trained model is saved to a specified directory.\n",
        "- **`predict_folds`**: Performs predictions for all train-test folds, iterating over the splits and calling `predict_single_split` for each fold. It saves the results for all folds, including a summary of Pearson correlations, L2 errors and R² scores.\n",
        "- **`run_training`**: Executes the full training process by calling `predict_folds` and saving the encoder performance results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HistologyGeneDataset(Dataset):\n",
        "    def __init__(self, h5_file, transform=None):\n",
        "        self.h5_file = h5_file\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(h5_file, 'r') as f:\n",
        "            self.images = f['images'][:]  # (N, 24, 24, 3)\n",
        "            self.targets = f['targets'][:]  # (N, num_genes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Transformations for data augmentation and resizing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # Resize to fit ResNet input\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# # Initialize dataset and dataloader\n",
        "# train_dataset = HistologyGeneDataset('/path/to/train.h5', transform=transform)\n",
        "# val_dataset = HistologyGeneDataset('/path/to/val.h5', transform=transform)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "DTgP9NPwc4w5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YI7ySRvP2-qs"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# Training functions\n",
        "##########\n",
        "\n",
        "def predict_single_split(fold, train_split, test_split, args, save_dir, model_name, device, bench_data_root):\n",
        "    \"\"\" Predict a single split for a single model \"\"\"\n",
        "\n",
        "    # Ensure paths to train and test split files are correct\n",
        "    if not os.path.isfile(train_split):\n",
        "        train_split = os.path.join(bench_data_root, 'splits', train_split)\n",
        "\n",
        "    if not os.path.isfile(test_split):\n",
        "        test_split = os.path.join(bench_data_root, 'splits', test_split)\n",
        "\n",
        "    # Read train and test split CSV files\n",
        "    train_df = pd.read_csv(train_split)\n",
        "    test_df = pd.read_csv(test_split)\n",
        "\n",
        "    # Directory to save embedding results\n",
        "    embedding_dir = args.embed_dataroot\n",
        "    os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "    # Embedding process\n",
        "    print(\"\\n--EMBEDDING--\\n\")\n",
        "    print(f\"Embedding tiles using {model_name} encoder\")\n",
        "    encoder: InferenceEncoder = inf_encoder_factory(model_name)(args.weights_root)\n",
        "\n",
        "    # Loop over train and test splits to generate embeddings\n",
        "    for split in [train_df, test_df]:\n",
        "        for i in range(len(split)):\n",
        "            sample_id = split.iloc[i]['sample_id']\n",
        "            tile_h5_path = os.path.join(bench_data_root, split.iloc[i]['patches_path'])\n",
        "            assert os.path.isfile(tile_h5_path)\n",
        "            embed_path = os.path.join(embedding_dir, f'{sample_id}.h5')\n",
        "            print(f\"\\nGENERATE EMBEDDING - {sample_id}\\n\")\n",
        "            generate_embeddings(embed_path, encoder, device, tile_h5_path, args.batch_size, args.num_workers, overwrite=args.overwrite)\n",
        "\n",
        "    # Initialize dictionary to hold data for both splits\n",
        "    all_split_assets = {}\n",
        "\n",
        "    gene_list = args.gene_list\n",
        "\n",
        "    # print(f'using gene_list {gene_list}')\n",
        "    # Load gene list for expression data\n",
        "    with open(os.path.join(bench_data_root, gene_list), 'r') as f:\n",
        "        genes = json.load(f)['genes']\n",
        "\n",
        "    # Process train and test splits\n",
        "    for split_key, split in zip(['train', 'test'], [train_df, test_df]):\n",
        "        split_assets = {}\n",
        "        for i in tqdm(range(len(split))):\n",
        "            # Get sample ID, embedding path and gene expression path\n",
        "            sample_id = split.iloc[i]['sample_id']\n",
        "            embed_path = os.path.join(embedding_dir, f'{sample_id}.h5')\n",
        "            expr_path = os.path.join(bench_data_root, split.iloc[i]['expr_path'])\n",
        "\n",
        "            # Read embedding data and gene expression data\n",
        "            assets, _ = read_assets_from_h5(embed_path)\n",
        "            barcodes = assets['barcodes'].flatten().astype(str).tolist()\n",
        "            adata = load_adata(expr_path, genes=genes, barcodes=barcodes, normalize=args.normalize)\n",
        "            assets['adata'] = adata.values\n",
        "\n",
        "            # Merge assets for the split\n",
        "            split_assets = merge_dict(split_assets, assets)\n",
        "\n",
        "        # Concatenate all data in the split\n",
        "        for key, val in split_assets.items():\n",
        "            split_assets[key] = np.concatenate(val, axis=0)\n",
        "\n",
        "        all_split_assets[split_key] = split_assets\n",
        "        print(f\"Loaded {split_key} split with {len(split_assets['embeddings'])} samples: {split_assets['embeddings'].shape}\")\n",
        "\n",
        "    # Assign data to X and y variables for training and testing\n",
        "    X_train, y_train = all_split_assets['train']['embeddings'], all_split_assets['train']['adata']\n",
        "    X_test, y_test = all_split_assets['test']['embeddings'], all_split_assets['test']['adata']\n",
        "\n",
        "    print(\"\\n--REGRESSION--\\n\")\n",
        "    # Perform regression using the specified method\n",
        "    reg, probe_results = train_test_reg(X_train, X_test, y_train, y_test, random_state=args.seed, genes=genes, method=args.method)\n",
        "\n",
        "    # Save the trained regression model\n",
        "    model_path = os.path.join(save_dir, f'model.pkl')\n",
        "    joblib.dump(reg, model_path)\n",
        "    print(f\"Model saved in '{model_path}'\")\n",
        "\n",
        "    # Summarize results for the current fold\n",
        "    print(f\"\\n--FOLD {fold} RESULTS--\\n\")\n",
        "    probe_summary = {}\n",
        "    probe_summary.update({'n_train': len(y_train), 'n_test': len(y_test)})\n",
        "    probe_summary.update({key: val for key, val in probe_results.items()})\n",
        "    keys_to_print = [\"n_train\", \"n_test\", \"pearson_mean\", \"l2_errors_mean\", \"r2_scores_mean\", \"l2_error_q1\", \"l2_error_q2\", \"l2_error_q3\"]\n",
        "\n",
        "    filtered_summary = {\n",
        "        key: round(probe_summary[key], 4)\n",
        "        for key in keys_to_print\n",
        "        if key in probe_summary\n",
        "    }\n",
        "    print(filtered_summary)\n",
        "\n",
        "    with open(os.path.join(save_dir, f'results.json'), 'w') as f:\n",
        "        json.dump(probe_results, f, sort_keys=True, indent=4)\n",
        "\n",
        "    with open(os.path.join(save_dir, f'summary.json'), 'w') as f:\n",
        "        json.dump(probe_summary, f, sort_keys=True, indent=4)\n",
        "\n",
        "    return probe_results\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "\n",
        "def predict_single_split_finetuning(fold, train_split, test_split, args, save_dir, model_name, device, bench_data_root):\n",
        "    \"\"\" Predict a single split for a single model\n",
        "        Resnet50 weights are fine tuning for histopathology images\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure paths to train and test split files are correct\n",
        "    if not os.path.isfile(train_split):\n",
        "        train_split = os.path.join(bench_data_root, 'splits', train_split)\n",
        "\n",
        "    if not os.path.isfile(test_split):\n",
        "        test_split = os.path.join(bench_data_root, 'splits', test_split)\n",
        "\n",
        "    # Read train and test split CSV files\n",
        "    train_df = pd.read_csv(train_split)\n",
        "    test_df = pd.read_csv(test_split)\n",
        "\n",
        "    # # Directory to save embedding results\n",
        "    # embedding_dir = args.embed_dataroot\n",
        "    # os.makedirs(embedding_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    # Loop over train and test splits to make sure all tiles are created\n",
        "    for split in [train_df, test_df]:\n",
        "        for i in range(len(split)):\n",
        "            sample_id = split.iloc[i]['sample_id']\n",
        "            tile_h5_path = os.path.join(bench_data_root, split.iloc[i]['patches_path'])\n",
        "            assert os.path.isfile(tile_h5_path)\n",
        "            # embed_path = os.path.join(embedding_dir, f'{sample_id}.h5')\n",
        "            # print(f\"\\nGENERATE EMBEDDING - {sample_id}\\n\")\n",
        "            # generate_embeddings(embed_path, encoder, device, tile_h5_path, args.batch_size, args.num_workers, overwrite=args.overwrite)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize dictionary to hold data for both splits\n",
        "    all_split_assets = {}\n",
        "\n",
        "    gene_list = args.gene_list\n",
        "\n",
        "    # print(f'using gene_list {gene_list}')\n",
        "    # Load gene list for expression data\n",
        "    with open(os.path.join(bench_data_root, gene_list), 'r') as f:\n",
        "        genes = json.load(f)['genes']\n",
        "\n",
        "    # Process train and test splits\n",
        "    for split_key, split in zip(['train', 'test'], [train_df, test_df]):\n",
        "        split_assets = {}\n",
        "        for i in tqdm(range(len(split))):\n",
        "            # Get sample ID, tile path and gene expression path\n",
        "            sample_id = split.iloc[i]['sample_id']\n",
        "            tile_h5_path = os.path.join(bench_data_root, split.iloc[i]['patches_path'])\n",
        "            expr_path = os.path.join(bench_data_root, split.iloc[i]['expr_path'])\n",
        "\n",
        "            # Read embedding data and gene expression data\n",
        "            assets, _ = read_assets_from_h5(tile_h5_path)\n",
        "            barcodes = assets['barcodes'].flatten().astype(str).tolist()\n",
        "            adata = load_adata(expr_path, genes=genes, barcodes=barcodes, normalize=args.normalize)\n",
        "            assets['adata'] = adata.values\n",
        "\n",
        "            # Merge assets for the split\n",
        "            split_assets = merge_dict(split_assets, assets)\n",
        "\n",
        "        # Concatenate all data in the split\n",
        "        for key, val in split_assets.items():\n",
        "            split_assets[key] = np.concatenate(val, axis=0)\n",
        "\n",
        "        all_split_assets[split_key] = split_assets\n",
        "        print(f\"Loaded {split_key} split with {len(split_assets['img'])} samples: {split_assets['img'].shape}\")\n",
        "\n",
        "    # Assign data to X and y variables for training and testing\n",
        "    X_train, y_train = all_split_assets['train']['img'], all_split_assets['train']['adata']\n",
        "    X_test, y_test = all_split_assets['test']['img'], all_split_assets['test']['adata']\n",
        "\n",
        "    print(\"\\n--REGRESSION--\\n\")\n",
        "    # Perform regression using the specified method\n",
        "    # reg, probe_results = train_test_reg(X_train, X_test, y_train, y_test, random_state=args.seed, genes=genes, method=args.method)\n",
        "\n",
        "\n",
        "\n",
        "    encoder = timm.create_model('resnet50', pretrained=True)\n",
        "    # Modify the final fully connected layer\n",
        "    num_genes = 460  # Number of genes to predict\n",
        "    in_features = encoder.get_classifier().in_features\n",
        "    encoder.fc = nn.Linear(in_features, num_genes)  # Replace with regression head\n",
        "\n",
        "    # Move model to GPU\n",
        "    encoder.to(device)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.MSELoss()  # Regression task\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 10\n",
        "    batch_size = 128\n",
        "    for epoch in range(num_epochs):\n",
        "      encoder.train()\n",
        "      train_loss = 0.0\n",
        "\n",
        "      # Training\n",
        "      for start_idx in range(0, len(X_train), batch_size):\n",
        "          end_idx = min(start_idx + batch_size, len(X_train))\n",
        "          # for images, targets in tqdm(zip(X_train, y_train), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "          images, targets = X_train[start_idx:end_idx].to(device), y_train[start_idx:end_idx].to(device)\n",
        "\n",
        "          # Zero gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = encoder(transform(images))\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss += loss.item()\n",
        "\n",
        "      # Validation\n",
        "      encoder.eval()\n",
        "      val_loss = 0.0\n",
        "      with torch.no_grad():\n",
        "        outputs = encoder(transform(X_test))\n",
        "        loss = criterion(outputs, y_test)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "      # Scheduler step\n",
        "      scheduler.step()\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "            f\"Train Loss: {train_loss/(len(X_train)//batch_size):.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # # Save the trained regression model\n",
        "    # model_path = os.path.join(save_dir, f'model.pkl')\n",
        "    # joblib.dump(reg, model_path)\n",
        "    # print(f\"Model saved in '{model_path}'\")\n",
        "\n",
        "    # # Summarize results for the current fold\n",
        "    # print(f\"\\n--FOLD {fold} RESULTS--\\n\")\n",
        "    # probe_summary = {}\n",
        "    # probe_summary.update({'n_train': len(y_train), 'n_test': len(y_test)})\n",
        "    # probe_summary.update({key: val for key, val in probe_results.items()})\n",
        "    # keys_to_print = [\"n_train\", \"n_test\", \"pearson_mean\", \"l2_errors_mean\", \"r2_scores_mean\", \"l2_error_q1\", \"l2_error_q2\", \"l2_error_q3\"]\n",
        "\n",
        "    # filtered_summary = {\n",
        "    #     key: round(probe_summary[key], 4)\n",
        "    #     for key in keys_to_print\n",
        "    #     if key in probe_summary\n",
        "    # }\n",
        "    # print(filtered_summary)\n",
        "\n",
        "    # with open(os.path.join(save_dir, f'results.json'), 'w') as f:\n",
        "    #     json.dump(probe_results, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # with open(os.path.join(save_dir, f'summary.json'), 'w') as f:\n",
        "    #     json.dump(probe_summary, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # return probe_results\n",
        "\n",
        "\n",
        "def predict_folds(args, exp_save_dir, model_name, device, bench_data_root):\n",
        "    \"\"\" Predict all folds for a given model \"\"\"\n",
        "\n",
        "    # Define the directory for splits\n",
        "    split_dir = os.path.join(bench_data_root, 'splits')\n",
        "    if not os.path.exists(split_dir):\n",
        "        raise FileNotFoundError(f\"{split_dir} doesn't exist, make sure that you specified the splits directory\")\n",
        "\n",
        "    splits = os.listdir(split_dir)\n",
        "    if len(splits) == 0:\n",
        "        raise FileNotFoundError(f\"{split_dir} is empty, make sure that you specified train and test files\")\n",
        "\n",
        "    n_splits = len(splits) // 2\n",
        "\n",
        "    # Save training configuration to JSON\n",
        "    with open(os.path.join(exp_save_dir, 'config.json'), 'w') as f:\n",
        "        json.dump(vars(args), f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Loop through each split and perform predictions\n",
        "    libprobe_results_arr = []\n",
        "    for i in range(n_splits):\n",
        "        print(f\"\\n--FOLD {i}--\\n\")\n",
        "        train_split = os.path.join(split_dir, f'train_{i}.csv')\n",
        "        test_split = os.path.join(split_dir, f'test_{i}.csv')\n",
        "        kfold_save_dir = os.path.join(exp_save_dir, f'split{i}')\n",
        "        os.makedirs(kfold_save_dir, exist_ok=True)\n",
        "\n",
        "        # Predict using the current fold\n",
        "        linprobe_results = predict_single_split(i, train_split, test_split, args, kfold_save_dir, model_name, device=device, bench_data_root=bench_data_root)\n",
        "        libprobe_results_arr.append(linprobe_results)\n",
        "\n",
        "    # Merge and save k-fold results\n",
        "    kfold_results = merge_fold_results(libprobe_results_arr)\n",
        "    with open(os.path.join(exp_save_dir, f'results_kfold.json'), 'w') as f:\n",
        "        p_corrs = kfold_results['pearson_corrs']\n",
        "        p_corrs = sorted(p_corrs, key=itemgetter('mean'), reverse=True)\n",
        "        kfold_results['pearson_corrs'] = p_corrs\n",
        "        json.dump(kfold_results, f, sort_keys=True, indent=4)\n",
        "\n",
        "    return kfold_results\n",
        "\n",
        "\n",
        "def predict_folds_finetuning(args, exp_save_dir, model_name, device, bench_data_root):\n",
        "    \"\"\" Predict all folds for a given model \"\"\"\n",
        "\n",
        "    # Define the directory for splits\n",
        "    split_dir = os.path.join(bench_data_root, 'splits')\n",
        "    if not os.path.exists(split_dir):\n",
        "        raise FileNotFoundError(f\"{split_dir} doesn't exist, make sure that you specified the splits directory\")\n",
        "\n",
        "    splits = os.listdir(split_dir)\n",
        "    if len(splits) == 0:\n",
        "        raise FileNotFoundError(f\"{split_dir} is empty, make sure that you specified train and test files\")\n",
        "\n",
        "    n_splits = len(splits) // 2\n",
        "\n",
        "    # Save training configuration to JSON\n",
        "    with open(os.path.join(exp_save_dir, 'config.json'), 'w') as f:\n",
        "        json.dump(vars(args), f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Loop through each split and perform predictions\n",
        "    libprobe_results_arr = []\n",
        "    for i in range(n_splits):\n",
        "        print(f\"\\n--FOLD {i}--\\n\")\n",
        "        train_split = os.path.join(split_dir, f'train_{i}.csv')\n",
        "        test_split = os.path.join(split_dir, f'test_{i}.csv')\n",
        "        kfold_save_dir = os.path.join(exp_save_dir, f'split{i}')\n",
        "        os.makedirs(kfold_save_dir, exist_ok=True)\n",
        "\n",
        "        # Predict using the current fold\n",
        "        predict_single_split_finetuning(i, train_split, test_split, args, kfold_save_dir, model_name, device=device, bench_data_root=bench_data_root)\n",
        "\n",
        "\n",
        "def run_training(args):\n",
        "    \"\"\" Training function: Execute predict_folds for processed train dataset with the specified encoder and dump the results in a nested directory structure \"\"\"\n",
        "\n",
        "    print(\"\\n-- RUN TRAINING ---------------------------------------------------------------\\n\")\n",
        "\n",
        "    print(f'run parameters {args}')\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create directory for results\n",
        "    save_dir = args.results_dir\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Perform predictions for all folds\n",
        "    enc_results = predict_folds(args, save_dir, model_name=args.encoder, device=device, bench_data_root=args.dir_dataset)\n",
        "\n",
        "    # Store and save encoder performance results\n",
        "    enc_perfs = {\n",
        "        'encoder_name': args.encoder,\n",
        "        'pearson_mean': round(enc_results['pearson_mean'], 4),\n",
        "        'pearson_std': round(enc_results['pearson_std'], 4),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'dataset_results.json'), 'w') as f:\n",
        "        json.dump(enc_perfs, f, sort_keys=True, indent=4)\n",
        "\n",
        "    print(\"\\n-- TRAINING DONE ---------------------------------------------------------------\\n\")\n",
        "\n",
        "    print(\"\\n-- Leave-one-out CV performance ---------------------------------------------------------------\")\n",
        "    print(enc_perfs)\n",
        "\n",
        "def run_training_finetuning(args):\n",
        "    \"\"\" Training function: Execute predict_folds for processed train dataset with the specified encoder and dump the results in a nested directory structure \"\"\"\n",
        "\n",
        "    print(\"\\n-- RUN TRAINING ---------------------------------------------------------------\\n\")\n",
        "\n",
        "    print(f'run parameters {args}')\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create directory for results\n",
        "    save_dir = args.results_dir\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Perform predictions for all folds\n",
        "    predict_folds_finetuning(args, save_dir, model_name=args.encoder, device=device, bench_data_root=args.dir_dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLCzt4NwsqV6"
      },
      "source": [
        "## Train function\n",
        "\n",
        "The `train` function automates the training of models for spatial transcriptomics data.\n",
        "\n",
        "1. Prepare the input data by preprocessing the spatial transcriptomics datasets (image patches (X) and gene expression (Y)).\n",
        "2. Create leave-one-out cross-validation splits for training and testing.\n",
        "3. Use the specified encoder model (ResNet50) and a regression method (ridge regression) to train the model\n",
        "4. Save the results and the trained model in the `.resources/` directories for later inference.\n",
        "\n",
        "![train_architecture](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/train_architecture.png)\n",
        "\n",
        "`Y` is log1p-normalized with scale factor 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PVnex5Qo-pUl"
      },
      "outputs": [],
      "source": [
        "# In the training function, users build and train the model to make inferences on the test data.\n",
        "# Your train models must be stored in the `model_directory_path`.\n",
        "def train(\n",
        "    data_directory_path: str, # Path to the input data directory\n",
        "    model_directory_path: str, # Path to save the trained model and results\n",
        "    size_subset: int = None, # Sampling size_subset smaller images from each H&E image\n",
        "    weights_root: str = None, # Path to the pre-trained model weights\n",
        "    target_patch_size: int = 32,  # Target size of cell patches (sub-region of an image) for the data\n",
        "    fine_tuning: bool = False,  # Whether to fine-tune the model (ResNet50) or not\n",
        "):\n",
        "    print(\"\\n-- TRAINING ---------------------------------------------------------------\\n\")\n",
        "\n",
        "    os.makedirs(model_directory_path, exist_ok=True)\n",
        "\n",
        "    # Directory to store the processed train dataset (temporary storage)\n",
        "    dir_processed_dataset = os.path.join(\"/tmp\", f\"processed_dataset\")\n",
        "    os.makedirs(dir_processed_dataset, exist_ok=True)\n",
        "\n",
        "    # Directory to store models and results\n",
        "    dir_models_and_results = os.path.join(model_directory_path, f\"ST_pred_results\")\n",
        "    os.makedirs(dir_models_and_results, exist_ok=True)\n",
        "\n",
        "    # List of datasets for Spatial Transcriptomics (ST) training\n",
        "    # The DC1 sample is deliberately absent because it does not contain spatial transcriptomic data.\n",
        "    list_ST_name_data = [\"UC1_NI\", \"UC1_I\", \"UC6_NI\", \"UC6_I\", \"UC7_I\", \"UC9_I\", \"DC5\"]\n",
        "\n",
        "    # Training parameters\n",
        "    args_dict = {\n",
        "        # Parameters for data preprocessing\n",
        "        \"size_subset\": size_subset,  # Sampling 10,000 smaller images from each H&E image\n",
        "        \"target_patch_size\": target_patch_size,  # Target size of cell patches (sub-region of an image) for the data\n",
        "\n",
        "        \"show_extracted_images\": False,  # (Only for visualization) Whether to visualize all the extracted patches for the first ST data\n",
        "        \"vis_width\": 1000,  # (Only for visualization) Width of the above visualization\n",
        "\n",
        "        # Generic training settings\n",
        "        \"seed\": 1,  # Random seed for reproducibility\n",
        "        \"overwrite\": False,  # Whether to overwrite the existing embedding files (Set to True if you have just changed some data preprocessing parameters)\n",
        "        \"dir_dataset\": dir_processed_dataset,  # Path to save the processed train dataset\n",
        "        \"embed_dataroot\": os.path.join(dir_processed_dataset, f\"ST_data_emb_size_subset_{size_subset}_patch_size_{target_patch_size}\"),  # Path for embedding data\n",
        "        \"results_dir\": dir_models_and_results,  # Directory to save results\n",
        "        \"n_fold\": 2,  # Number of folds for leave-one-out cross-validation\n",
        "\n",
        "        # Encoder settings\n",
        "        \"batch_size\": 128,  # Batch size for training\n",
        "        \"num_workers\": 0,  # Number of workers for loading data (set 0 for submission on CrunchDAO plaform)\n",
        "\n",
        "        # Train settings\n",
        "        \"gene_list\": \"var_genes.json\",  # Path to save the list of genes\n",
        "        \"method\": \"ridge\",  # Regression method to use\n",
        "        \"alpha\": None,  # Regularization parameter for ridge regression\n",
        "        \"normalize\": False,  # Whether to normalize the data (sdata[\"anucleus\"].X is already normalized with scale factor 100)\n",
        "        \"encoder\": \"resnet50\",  # Encoder model to use (e.g., ResNet50)\n",
        "        \"weights_root\": weights_root  # Path to the pre-trained model weights\n",
        "    }\n",
        "\n",
        "    # Convert the dictionary to a simple namespace for easy access to attributes\n",
        "    args = SimpleNamespace(**args_dict)\n",
        "\n",
        "    # Preprocess the spatial transcriptomics data for each sample in list_ST_name_data -> all preprocessed data saved in dir_processed_dataset\n",
        "    preprocess_spatial_transcriptomics_data_train(list_ST_name_data, data_directory_path, dir_processed_dataset,\n",
        "                                                  args.size_subset, args.target_patch_size, args.vis_width, args.show_extracted_images)\n",
        "\n",
        "    # Create leave-one-out cross-validation splits for training and testing in csv files\n",
        "    create_cross_validation_splits(dir_processed_dataset, n_fold=args.n_fold)\n",
        "\n",
        "    # Run the training process using the parameters set above\n",
        "    if not fine_tuning:\n",
        "        run_training(args)\n",
        "    else:\n",
        "        run_training_finetuning(args)\n",
        "    # run_training(args)\n",
        "\n",
        "    # View all results in directory `args.results_dir`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CokrevgcHdm"
      },
      "source": [
        "All regression models and metric results are saved in `./resources/ST_pred_results` -- check it out!\n",
        "\n",
        "Preprocessed datasets are saved in `/tmp/processed_dataset` (temporary storage - not needed for inference).\n",
        "\n",
        "Official challenge metric: L2 mean error (`l2_errors_mean`)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ./resources/pytorch_model.bin https://huggingface.co/timm/resnet50.tv_in1k/resolve/main/pytorch_model.bin?download=true"
      ],
      "metadata": {
        "id": "NZqzEiMsU3IM",
        "outputId": "9af859b6-2254-4deb-ffae-e617f3d1ddc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 21:59:24--  https://huggingface.co/timm/resnet50.tv_in1k/resolve/main/pytorch_model.bin?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.80, 18.239.50.49, 18.239.50.103, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/d7/67/d7671a77b74208d1ef99326d57fa2adcb3cb994975c6b9be0d7e494fc3e55d8b/065b941a162ea3d2ef6b4b5c3f13b8ccdf8b761496d2ee334ad9b09cbca3c4f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1736287164&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNjI4NzE2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kNy82Ny9kNzY3MWE3N2I3NDIwOGQxZWY5OTMyNmQ1N2ZhMmFkY2IzY2I5OTQ5NzVjNmI5YmUwZDdlNDk0ZmMzZTU1ZDhiLzA2NWI5NDFhMTYyZWEzZDJlZjZiNGI1YzNmMTNiOGNjZGY4Yjc2MTQ5NmQyZWUzMzRhZDliMDljYmNhM2M0Zjg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=ucX1SSiZC1p7jC%7E3og3%7ElnmYvD1bpzxvHqyPOksn6wFTfGjFHX2mxpqcMgsFeDha2offyIJv6htg9dotcnYRdJltaQwc9kOwVBRHigPRv8TYCveiwVF4JHqjRoFlgB32-sZMUSs31p42MF7qqaUWpabfEpH%7EXigliXrIsHPMr%7En-ccFpNlWRD66sac5HgpsAZQfJRIlF7haKk4-ziJcbBE%7Er5VbUoszw0swzLtzEO4rnubNL96uCthqnqUf7RLB3J6G2ZwlqDJ72DqxzVJE3DJTFX56mk5fgPXKlSZx1xvNrH4Wxm6xesbDP0jXyhuLwhB0cTRIHV6Fk9CterVp1QA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-01-04 21:59:24--  https://cdn-lfs.hf.co/repos/d7/67/d7671a77b74208d1ef99326d57fa2adcb3cb994975c6b9be0d7e494fc3e55d8b/065b941a162ea3d2ef6b4b5c3f13b8ccdf8b761496d2ee334ad9b09cbca3c4f8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1736287164&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNjI4NzE2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kNy82Ny9kNzY3MWE3N2I3NDIwOGQxZWY5OTMyNmQ1N2ZhMmFkY2IzY2I5OTQ5NzVjNmI5YmUwZDdlNDk0ZmMzZTU1ZDhiLzA2NWI5NDFhMTYyZWEzZDJlZjZiNGI1YzNmMTNiOGNjZGY4Yjc2MTQ5NmQyZWUzMzRhZDliMDljYmNhM2M0Zjg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=ucX1SSiZC1p7jC%7E3og3%7ElnmYvD1bpzxvHqyPOksn6wFTfGjFHX2mxpqcMgsFeDha2offyIJv6htg9dotcnYRdJltaQwc9kOwVBRHigPRv8TYCveiwVF4JHqjRoFlgB32-sZMUSs31p42MF7qqaUWpabfEpH%7EXigliXrIsHPMr%7En-ccFpNlWRD66sac5HgpsAZQfJRIlF7haKk4-ziJcbBE%7Er5VbUoszw0swzLtzEO4rnubNL96uCthqnqUf7RLB3J6G2ZwlqDJ72DqxzVJE3DJTFX56mk5fgPXKlSZx1xvNrH4Wxm6xesbDP0jXyhuLwhB0cTRIHV6Fk9CterVp1QA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.239.83.30, 18.239.83.10, 18.239.83.87, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.239.83.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102545229 (98M) [application/octet-stream]\n",
            "Saving to: ‘./resources/pytorch_model.bin’\n",
            "\n",
            "./resources/pytorch 100%[===================>]  97.79M  55.5MB/s    in 1.8s    \n",
            "\n",
            "2025-01-04 21:59:26 (55.5 MB/s) - ‘./resources/pytorch_model.bin’ saved [102545229/102545229]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mre9PhC9AQMl",
        "outputId": "821426d7-9ce1-4948-b759-83b4ee1eff34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- TRAINING ---------------------------------------------------------------\n",
            "\n",
            "\n",
            " -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\n",
            "\n",
            "\n",
            "DATA (1/7): UC1_NI\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 93686/93686 [00:07<00:00, 11782.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (2/7): UC1_I\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234356/234356 [00:27<00:00, 8562.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (3/7): UC6_NI\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123517/123517 [00:09<00:00, 12731.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (4/7): UC6_I\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 308924/308924 [00:36<00:00, 8359.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (5/7): UC7_I\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 277046/277046 [00:25<00:00, 10872.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (6/7): UC9_I\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 244527/244527 [00:26<00:00, 9361.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "\n",
            "DATA (7/7): DC5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling the dataset ...\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 171019/171019 [00:15<00:00, 11126.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create gene expression dataset (Y) ...\n",
            "H5 file already exists\n",
            "Save gene list in /tmp/processed_dataset/var_genes.json\n",
            "Total number of genes: 460\n",
            "\n",
            "Preprocess dataset DONE: UC1_NI - UC1_I - UC6_NI - UC6_I - UC7_I - UC9_I - DC5 \n",
            "\n",
            "\n",
            " -- CREATE CROSS-VALIDATION SPLITS --------------------------------------------\n",
            "\n",
            "Index 0:\n",
            "Train DataFrame:\n",
            "  sample_id       patches_path          expr_path\n",
            "1     UC1_I   patches/UC1_I.h5   adata/UC1_I.h5ad\n",
            "2     UC6_I   patches/UC6_I.h5   adata/UC6_I.h5ad\n",
            "3    UC1_NI  patches/UC1_NI.h5  adata/UC1_NI.h5ad\n",
            "4    UC6_NI  patches/UC6_NI.h5  adata/UC6_NI.h5ad\n",
            "5     UC7_I   patches/UC7_I.h5   adata/UC7_I.h5ad\n",
            "6     UC9_I   patches/UC9_I.h5   adata/UC9_I.h5ad\n",
            "Test DataFrame:\n",
            "  sample_id    patches_path       expr_path\n",
            "0       DC5  patches/DC5.h5  adata/DC5.h5ad\n",
            "Saved train_0.csv and test_0.csv\n",
            "Index 1:\n",
            "Train DataFrame:\n",
            "  sample_id       patches_path          expr_path\n",
            "0       DC5     patches/DC5.h5     adata/DC5.h5ad\n",
            "2     UC6_I   patches/UC6_I.h5   adata/UC6_I.h5ad\n",
            "3    UC1_NI  patches/UC1_NI.h5  adata/UC1_NI.h5ad\n",
            "4    UC6_NI  patches/UC6_NI.h5  adata/UC6_NI.h5ad\n",
            "5     UC7_I   patches/UC7_I.h5   adata/UC7_I.h5ad\n",
            "6     UC9_I   patches/UC9_I.h5   adata/UC9_I.h5ad\n",
            "Test DataFrame:\n",
            "  sample_id      patches_path         expr_path\n",
            "1     UC1_I  patches/UC1_I.h5  adata/UC1_I.h5ad\n",
            "Saved train_1.csv and test_1.csv\n",
            "\n",
            "-- RUN TRAINING ---------------------------------------------------------------\n",
            "\n",
            "run parameters namespace(size_subset=10000, target_patch_size=32, show_extracted_images=False, vis_width=1000, seed=1, overwrite=False, dir_dataset='/tmp/processed_dataset', embed_dataroot='/tmp/processed_dataset/ST_data_emb_size_subset_10000_patch_size_32', results_dir='./resources/ST_pred_results', n_fold=2, batch_size=128, num_workers=0, gene_list='var_genes.json', method='ridge', alpha=None, normalize=False, encoder='resnet50', weights_root='./resources/pytorch_model.bin')\n",
            "\n",
            "--FOLD 0--\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'barcodes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-53d0db6eb19b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# /!\\ Don't forget to import pre-trained Resnet50 \"pytorch_model.bin\" in model_directory_path=\"./resources\" from https://huggingface.co/timm/resnet50.tv_in1k/tree/main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_directory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./resources\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./resources\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"pytorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-75ae8d830e9e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_directory_path, model_directory_path, size_subset, weights_root, target_patch_size, fine_tuning)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mrun_training_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;31m# run_training(args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a2ff2b6f8620>\u001b[0m in \u001b[0;36mrun_training_finetuning\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;31m# Perform predictions for all folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0mpredict_folds_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbench_data_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a2ff2b6f8620>\u001b[0m in \u001b[0;36mpredict_folds_finetuning\u001b[0;34m(args, exp_save_dir, model_name, device, bench_data_root)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Predict using the current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mpredict_single_split_finetuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbench_data_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbench_data_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a2ff2b6f8620>\u001b[0m in \u001b[0;36mpredict_single_split_finetuning\u001b[0;34m(fold, train_split, test_split, args, save_dir, model_name, device, bench_data_root)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# Read embedding data and gene expression data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0massets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_assets_from_h5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_h5_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mbarcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'barcodes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0madata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_adata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbarcodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbarcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0massets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'barcodes'"
          ]
        }
      ],
      "source": [
        "# /!\\ Don't forget to import pre-trained Resnet50 \"pytorch_model.bin\" in model_directory_path=\"./resources\" from https://huggingface.co/timm/resnet50.tv_in1k/tree/main\n",
        "train(data_directory_path='./data', model_directory_path=\"./resources\", size_subset=10000, weights_root = os.path.join(\"./resources\", f\"pytorch_model.bin\"), fine_tuning=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OSDlFhPEx3P"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "# functions for inference\n",
        "##########\n",
        "\n",
        "def embedding_and_load_data(name_data, dir_processed_dataset_test, test_embed_dir, args, device):\n",
        "    \"\"\"\n",
        "    Embedding of the images using the specified encoder and load the resulting data.\n",
        "\n",
        "    Args:\n",
        "    - name_data (str): The name of the data to process.\n",
        "    - dir_processed_dataset_test (str): Directory where the processed test dataset is stored.\n",
        "    - test_embed_dir (str): Directory where the embeddings should be saved.\n",
        "    - args (namespace): Arguments object containing parameters like encoder, batch_size, etc.\n",
        "    - device (torch.device): The device (CPU or GPU) to perform computations on.\n",
        "\n",
        "    Returns:\n",
        "    - assets (dict): Dictionary containing the 'barcodes', 'coords', and 'embeddings' from the embedded data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the encoder being used\n",
        "    print(f\"Embedding images using {args.encoder} encoder\")\n",
        "\n",
        "    # Create encoder based on the specified model and load its weights\n",
        "    encoder = inf_encoder_factory(args.encoder)(args.weights_root)\n",
        "\n",
        "    # Define the path for the patches data (h5 file)\n",
        "    tile_h5_path = os.path.join(dir_processed_dataset_test, \"patches\", f'{name_data}.h5')\n",
        "\n",
        "    # Check if the file exists\n",
        "    assert os.path.isfile(tile_h5_path), f\"Patches h5 file not found at {tile_h5_path}\"\n",
        "\n",
        "    # Define the embedding output path\n",
        "    embed_path = os.path.join(test_embed_dir, f'{name_data}.h5')\n",
        "\n",
        "    # Generate the embeddings and save them to the defined path\n",
        "    generate_embeddings(embed_path, encoder, device, tile_h5_path, args.batch_size, args.num_workers, overwrite=args.overwrite)\n",
        "\n",
        "    # Load the embeddings and related assets\n",
        "    assets, _ = read_assets_from_h5(embed_path)\n",
        "\n",
        "    # Extract cell IDs and convert to a list of strings\n",
        "    # The cell IDs are not necessary because the images are kept in the same order as the gene expression data\n",
        "    cell_ids = assets['barcodes'].flatten().astype(str).tolist()\n",
        "\n",
        "    return assets\n",
        "\n",
        "\n",
        "def load_models_from_directories(base_path):\n",
        "    \"\"\"\n",
        "    Load all trained regression models (one model for each cross-validation split)\n",
        "    Load 'model.pkl' from each directory within the base_path.\n",
        "\n",
        "    :param base_path: The parent directory containing split subdirectories.\n",
        "    :return: A dictionary where keys are directory names and values are the loaded models.\n",
        "    \"\"\"\n",
        "\n",
        "    models = {}\n",
        "    for name in os.listdir(base_path):\n",
        "        dir_path = os.path.join(base_path, name)\n",
        "        if os.path.isdir(dir_path):\n",
        "            model_path = os.path.join(dir_path, 'model.pkl')\n",
        "            if os.path.exists(model_path):\n",
        "                models[name] = joblib.load(model_path)\n",
        "                print(f\"Loaded model from {model_path}\")\n",
        "            else:\n",
        "                print(f\"'model.pkl' not found in {dir_path}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def predict_and_aggregate_models(X_test, results_dir):\n",
        "    \"\"\"\n",
        "    Load models from the given directory, make predictions on the test set (X_test),\n",
        "    aggregate the predictions by averaging, and set negative predictions to 0.\n",
        "\n",
        "    Args:\n",
        "    - X_test (np.array): The test data to make predictions on.\n",
        "    - results_dir (str): Directory containing the saved models.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: The aggregated predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load models from the specified directory\n",
        "    models = load_models_from_directories(results_dir)\n",
        "\n",
        "    # Initialize a list to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate through each model and make predictions\n",
        "    for split_name in models.keys():\n",
        "        preds = models[split_name].predict(X_test)\n",
        "        predictions.append(preds)\n",
        "\n",
        "    # Stack the predictions into a 2D array (models x samples)\n",
        "    predictions = np.stack(predictions)\n",
        "\n",
        "    # Aggregate predictions by calculating the mean across all models\n",
        "    average_predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Set any negative predictions to 0\n",
        "    average_predictions = np.where(average_predictions < 0, 0.0, average_predictions)\n",
        "\n",
        "    del models\n",
        "\n",
        "    return average_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6R_RAbOtxHh"
      },
      "source": [
        "## Infer function\n",
        "\n",
        "The `infer` function loads trained models and performs inference on a new dataset.\n",
        "\n",
        "1. Prepare the necessary directories and load the configuration parameters from the previously trained model.\n",
        "2. The test data, provided as a Zarr file, is read and specific subsets of the data (test and validation groups) are extracted.\n",
        "3. Preprocess the data into image patches (X_test).\n",
        "4. Generate embeddings for the test data and applies the trained models for regression predictions.\n",
        "5. Format the predictions for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40k_Y0a--9up"
      },
      "outputs": [],
      "source": [
        "# In the inference function, the trained model is loaded and used to make inferences on a\n",
        "# sample of data that matches the characteristics of the training test.\n",
        "def infer(\n",
        "    data_file_path: str,  # Path to a test dataset (in Zarr format) to perform inference on.\n",
        "    model_directory_path: str  # Path to save the trained model and results\n",
        "):\n",
        "    ### Prepare Directories ###\n",
        "\n",
        "    # Extract the name of the dataset from the file path (without extension)\n",
        "    name_data = os.path.splitext(os.path.basename(data_file_path))[0]\n",
        "    print(f\"\\n-- {name_data} INFERENCE ---------------------------------------------------------------\\n\")\n",
        "    print(data_file_path)\n",
        "\n",
        "    # Previous directory where models and results are stored\n",
        "    dir_models_and_results = os.path.join(model_directory_path, f\"ST_pred_results\")\n",
        "    # Load training configuration parameters\n",
        "    config_path = os.path.join(dir_models_and_results, \"config.json\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        args_dict = json.load(f)\n",
        "    args = SimpleNamespace(**args_dict)\n",
        "\n",
        "    # Directory for processed test dataset (temporary storage)\n",
        "    dir_processed_dataset_test = os.path.join(\"/tmp\", f\"processed_dataset_test\")\n",
        "    os.makedirs(dir_processed_dataset_test, exist_ok=True)\n",
        "\n",
        "    # Directory to store the test data embeddings (temporary storage)\n",
        "    test_embed_dir = os.path.join(dir_processed_dataset_test, \"ST_data_emb\")\n",
        "    os.makedirs(test_embed_dir, exist_ok=True)\n",
        "\n",
        "    # Set device to GPU if available, else use CPU!!\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ### Preprocess and Embedding Data + Regression inference ###\n",
        "\n",
        "    # Read the spatial data from the provided file\n",
        "    sdata = sd.read_zarr(data_file_path)\n",
        "\n",
        "    # Extract cell IDs for test and validation groups\n",
        "    cell_ids = list(sdata[\"cell_id-group\"].obs.query(\"group == 'test' or group == 'validation'\")[\"cell_id\"])\n",
        "\n",
        "    # Extract gene names from the spatial data\n",
        "    gene_names = list(sdata[\"anucleus\"].var.index)\n",
        "\n",
        "    # Preprocess the test data for embedding (patch extraction)\n",
        "    preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_ids, dir_processed_dataset_test,\n",
        "                                                 args.target_patch_size, args.vis_width, args.show_extracted_images)\n",
        "\n",
        "    print(f\"\\n-- {name_data} EMBEDDING--\\n\")\n",
        "    # Generate and load the embeddings for the test data\n",
        "    assets = embedding_and_load_data(name_data, dir_processed_dataset_test, test_embed_dir, args, device)\n",
        "\n",
        "    # Extract embeddings features for prediction\n",
        "    X_test = assets[\"embeddings\"]\n",
        "    print(\"Embedding shape (X_test):\", X_test.shape)\n",
        "\n",
        "    print(f\"\\n-- {name_data} REGRESSION PREDICTIONS--\\n\")\n",
        "    # Make predictions and aggregate results across cross-validation regression models\n",
        "    average_predictions = predict_and_aggregate_models(X_test, args.results_dir)\n",
        "\n",
        "    ### Prepare and Return Predictions ###\n",
        "\n",
        "    # Convert the predictions to a DataFrame (the gene expression value must be rounded to two decimal places)\n",
        "    prediction = pd.DataFrame(np.round(average_predictions, 2), index=cell_ids, columns=gene_names)\n",
        "    # Reset index to have 'cell_id' as a column\n",
        "    prediction = prediction.reset_index(names=\"cell_id\")\n",
        "\n",
        "    # Melt the DataFrame to the expected output for the challenge\n",
        "    prediction = prediction.melt(id_vars=\"cell_id\", var_name=\"gene\", value_name=\"prediction\")\n",
        "    # prediction = prediction.sort_values(by=[\"cell_id\", \"gene\"]).reset_index(drop=True)\n",
        "\n",
        "    # Free memory by deleting large variables and performing garbage collection\n",
        "    del average_predictions, sdata, X_test, assets\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n-- {name_data} PREDICTION DONE\\n\")\n",
        "\n",
        "    # Return the final prediction DataFrame\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v2T41EUZgau3",
        "outputId": "f9f4bbcb-67c3-4a55-b9e6-1148ce4493ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-- UC1_NI INFERENCE ---------------------------------------------------------------\n",
            "\n",
            "./data/UC1_NI.zarr\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\n",
            "\n",
            "Extracting spatial positions ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 93686/93686 [00:10<00:00, 9014.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading imgs ...\n",
            "Patching: create image dataset (X) ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13649/13649 [00:16<00:00, 827.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualization\n",
            "Spatial coordinates\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 636.364x666.667 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples from the created .h5 dataset\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAFpCAYAAADdvOLsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9D0lEQVR4nO3ayZIlZ5oe5s+Hc04MGZGRyMQ8VAFdQ08SSbXUJLWSTKRpqYVMWumapEvQJegWSC24YFOiuskeqlDoQk0NIDORkZkxnsHdteiWGavQMr5Owo1m5POsX/v8uPs/+RfRTNM0FQAAAAB8y9r/0D8AAAAAgP84aTwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACyiT4OfffaTvOpujGLbL27jkuO/eB1np798lQW3d/n131zn2TB6+ndP45r9Dx/H2eboJM7WOus97qcuLtkO+/z6Ye+zmVExG31/U3fKKjczig6HPNy+zrJ3P8vH6uH/Csd/VbU/+irK7b/8Iq453c6Yq7ttnh2zcTVOM15Wm2encLnc1hDX/G9//L/G2f8U/B//y/8WZ9/+brbOvf0//L245vj4PM5OQ7Z2tU0+HqZuFWe7mrLgmK+eQz9j7hzyuk1ziHLdlP8t7DDk12+77FmNTb7PNdvsnqqqapPVbWasR+OYj5Vp3GXBZsY7nTGuui7LDumYrqo6zHhXbT4HuzEcK8/jknX49CrONodwT7zI7/8H/+M/iLP/qfjJZz+Ks+m0GKf4k6ram3z9OIxZdv+v7+Oa0x9/nWevwzG5nbHX5Y+qmjfzb6X2SfYB1jw5jmuOX+brUn95HeWG2/w7qbnM3+u4z95BM+Ojpvkg//7d/DdvZzW/8yiuWX1+LhjDb7rpMn+nw5/l31T1Mv9WG55m2eHFTVyzuc3HSpuet9b5Xv/7//v//G+/blwNAAAAAGbQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBF9GpyaKa/aZLHuzfjyNb73IM5uv7iJcu16jGuuP+7i7NF3L6Jc99ZpXPNwtImzXfj8q6raQ1pziGsObf4DuikcV1Nes81/ak1TNgYOt3nR/fNdfv0fZ3XHH13GNbufP4uz91+/inL97i6u2VS+Vgz9jN73mM3BYcZYqW4VR6cm+62rNl/X+HU/+J9+K85Op0dRbrfO526/z8duuw5zM4ZjjfmeNI7h3AnXuKqqOoQbQlW1XfgAqmq6z+bENONhNc0+zo5T9qymyp/V1M/4rUNWd8qX2Wq6/P6ndXb/zYyxOic7hnvCnL2jm/GuZsyAGu+y9PjVnKr5u2rfyebKcG6f+fcx55jQTNmznrPWj20+frp1dvbZr/M1uU7zb5rxZbaHtof7vObrfK43N/nCOF2G30pPj/Pr7/Jz6ngIzxsvt3HN6TrPjrvs+6PZ5Pe0Gmd8/7/K1rppnHHWGGasdeFH7fgqn6z9lP/WsWbMqzDb7vN3dRjybLvKsoddvn9F1/1WqwEAAADA39B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvo42Qzo+o6C3d9fvnhe1OcXR0/iHLr7jiv+VaeHc43YdF1XLNv8x7hfsa76it7rt2MATCMY5w9jFndZszvv9kd4ux4mV1/+6P7uObh86s4Wz97meUuL/Prv76Os+02vK/DjHc6Y/w1XT4HxjZcL5ohrjnNaL0PJ+FvPenyovya1ccfxNluna3JTb511Njn765psoE+HfKaM5a56rpsnI/DjPHYzPgB9/mDnZpw/ZjxU/sZv3UX7jPdjAVhaPOFrrnN9qTpdf5M28f5b53SPXnGAGzb/LcO4eWnGe8/3Q6qqqa7GeeXz8J59fObuGbb7uLsdBKeNUf7zL+PsfIzTROeKeZ8Jk1HM7J3Ye52nxc9yu9/9ST7sYfX+dmrWeVzYpix1nRddl/T1YxndT/jvoZwrGzzmrWfMVbDxXYY8+ffbvPB2lxmz7X5xTauOXywirNtuIeNd/n16z6f2dOU153Owvf69YxD7JBfP/3+bmf0KqJ632o1AAAAAPgbGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACyiT4PtOMVFp7CdNVUT12wfbeLsyfmTKNc0Y1xzGPPfulpl2Wa1jms2Q/5b+/xVVU3Zb921Q1yyHfJnNYZlxxf3cc395/s8++kuyjWffxXXbF5dx9l6fRfFxvssV1W12ufPahqy+z/MmCtd28XZ3TofK9VnC0t7chyXnE5WcXb1OJuv3Rv59fkNq3hLqqnPxs444+8rfeWLZ7okp2tcVVUzY+1OV7lmxg9YVb4njats7aiqasI1oRny37pb5XO3CR9WW4e45rDNx9X+RfYD9p/dxDXX7+VnouE8yx61M+7/KI5WhWeirsn3jnHK96TxWX5f4/Ns/z7M2JOnl/lc6XbZItAczVhY+Iau8rHWVDbWxmHG3/K3M87JV+GY2M/4punzvbZ7L/ut/e9exDWnLn9WRzf5WL+/zNba4ct8TrZX+fWn622Ua8YZY2XOmT7MdUP+/iscf1VVh59m99++vIxrjn+Vr7Xjew+y4H2+J7Sn+fvvPsrPUP35SZQb3wnvqaqafxlHa7/N5sr2bk5T4d/OfzwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFtGnwWHq4qJtDVmuzfte+dWrqsvSU5tXXR8OcXacstxhHwarqmnHOFuV153GdZQb9/n9T1f5ex1+epfl/vR1XHP/i2dxtrsJr3+9jWs2d/dxdtxn73W/38U1u/0+zqb6VRNn932e7R8exdnuk7Ms+NZ5XHP9MBv/VVX1aBPF2tP8nvh1XZO/j3HMxlkzZz3s87WrH7K644zrD9nW+dfXP2Rrx6HL5+O4ydeZcZyxK4fb19Dmv7WbsScPYdlwOa6qqvZ+xvXDYTXd5APg8Mdfx9nxQbYmbZtVXLOO8nFd72ZrZ3eRz/9plb+s5umMFxvu9f1hxry+y9/r+IvrLNhkZxf+dnd3+fvrb8O9ZpW/5+kyv/7+09soNz7Pz8mbN+NoNZ9kZ6phE39SVs14/tP7edlVONemi3z+7P7iVZxtX2Xn//Y2v/5+l4+rvsuywyFf65vdTZwdn2f3dTjKrz8c59ndk2yvO34vP6cf/c6DONu8fRpnu9Nsv2s2+Vnj5ijf68avr6Lc9Kf5+0/4jycAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABbRp8F2dcirHrJ+1lBdXrOb4mhbWbbZj3HNoYsfVR2m7PqrMb/+eMjvv7mZ8Vxv7sLcEJe8+7Ntfv2fXGa5L5/HJdu78J6qqhmz5zoe9nnNGe9qOmTz6jDs4prtjHby2Ie/dbOOazZnqzh79MlZnO3/i3ey6791EdfsjvN5Pa422fXzR8VvWK3ywbsPl6Qmf8U1hGt3VdV0aLLgjAnZN/k6M6yz39rP2DubMd872i7P7g73Ua6f5kyefE/qwlc1zhgsTZ+fidqzbK/vHmVrTFXV9PPrOFufv4hih23+TKcZ86p58zTKjU9m7DNPjuPs+GU+B/rL7L0OtzPOBC/z/XsKF7amyc+PfNPtT7I1qarq6OVNlBuuZpy9n+fn5GmbZVdn+Zis9/KzV/swm5ftOl8U2os4WjM+lao9yxb79mTGWt+EG0hV1a+y74/hKh9/TZOvy+Hnd/V9XrMZ8hfQjulmm4//4T4/Q53tw7p9Pld2b+f7cvMg32tWm3BfeJivK/3vPYyz7ctsD3396uu4ZnTdb7UaAAAAAPwNjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABbRp8FpbOKibXXZxbtDXHMMa/5/6cQU331VN+S/dQgLD/v8nobXuzi7/9FdnG3+8usoN93O6FH+8jK//vVtlBu327zmMMTZQzNFuanJxlRV1TRjqE5h77fr1nHNrsvuqaqqOcvqri+O8+u/t8qzv/84zq5+61GUm07P45rtasa4nsJx1eT3z6+7z4duVZuF2yF/x03N2OfGfRY8zq8/TjPGYxOuCUO+dg2V73PdPq+7mbLnOrT58w9LVlVVE+7JMy5fY5vff7fONoX9Ol/n6zTfaMaX2drVHu7zmq9n7DM32ZlkutzENetpvic1uxlz8BCu8y/zM8l0nWfHXXbWazb+bvzvY/PL7OxZVXW4ytbF2+fXcc3Xz1/H2fGQjZ/T83xNeHyRn1OOHp9GuXbOXtfPGL9jvtasVtm6fOiO4pr9eb4x7Fcvolw75t9JNeNc1DTZGBgPedFZx7I0PeTjr9+FZ62qqjbLjk/zs872j/O5OvxVvoee/1fZ988q+/SpqqpunTc22uPw+++D/Jsquu63Wg0AAAAA/obGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABah8QQAAADAIjSeAAAAAFiExhMAAAAAi+jT4DQ1cdFxGsNgmKuqJq1ZVdV1YdG85NCt4mx/yH7r4eo+rjn9+S7O1o9extHm65soN7y+y2tezrivXZbt2yG//oyhsgtnQDujRzsd59efmuwHNGGuqqo9z7ObD0+y4Bvncc3+h2HNqlp/mNedjo+y66/yF3A3HuJsny6XTbj+8A195c9uarN3N01T/gPCtbuqaky3hEO+dhxqH2e7Pruvpsnvf5zm/C0qnztdm222U23jmtOMDbxpsv1jxpGgpmw5+utsuH1Ot/n7r6N8rK6eZD/28HrGPrvKzyRDm43Brsvvabqa8azuZ9zXEI6VbV6z9jPOukOWHcYZZ0K+Ydfn7+/16+yc+uMvL+Oan331Is42V8+i3Ad9vib/zvatOPveO78X5Y6+n38nNXO+Kft8XxrC7W5az9jrZpzp6yw7J07rdVxyvL2Ns/1NNgaG/DOhmhk74yE8b/XNjPV7Rq/gMGTrYrPLr7+ZsS/vb/MHewiny/DGaVyz/+Aszk6rbA60hxln6KTet1oNAAAAAP6GxhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYRJ8GmynvUTX9mAWnuGQ1M3pkY3j5tsmvX/dh0araX2e5wy+3cc3dn7+Ks80vX8TZ/d0+yvV3+W8dDvdxtmmz53qIK1Y144yBteqiWN9luaqqOsnHavvgJAuu8nvqPtrE2frPH2Y1zx7EJdvz4zjbrFZ5ts+e69jko6VrZoyVbKrUajNntPJvmsYhzjZNOM+G/B2P4XpUVdX02ZrQ5LdUqymfD4dDVribs8+OM+ZOu84Lhyt41+Vr5zTnTFLZex2HGX+L2+YHiPEqewnNPh9/Qx8f36p7L/ut/e9exDWnGe/q6CYbq/eX4SJbVcOXuzjbXuWTcLrOzjrNOGOs7PMzUTpduyF//3zT3Wm+fv346jLK/dPPv4hrvvr60zh7tM/O9C/6fE06bfNvios/PYtyq8encc1mRrZv8vuaxiw750g/PM7P1KvvZve1fxme/WvemXa8T9evGc90xjl5Gz7X9Nuvqmo9zdhr03PBjLPmdHMbZ4cpHyu7w1WUax/exDXb5/lvPTrLxmp3+e1+0/iPJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAi+jTYtENedWiiWNt2cclDn1+/O2R19+MY15wOeY9u/1d3UW748XVcc3zxMs52V7dxttmGv3WXP/92yp9rO2W5/SouWatVPq7GNpwCm3iqVP9wE2frnZMo1jxcxyW7N/PrD4/Psponp3HNdpW//yEdAFXVHMJsk60/VVXV5WOl+uy+tqN+/r+rccZ46MJ9ppo5a1ccrWaXhaf0d1bVcD9jPuyz+zq82uU1D/lv3Z3v42z3IFvAx00+H3dj/qz62+y+mlU+VqbL/Pr7T7M9eXz+Oq65eTOOVvPJeZQbZuxzdZff//R+llvdzXj+F9nZpapq9xev4mz76j7L3ebX3884P/Vdlh0OMw5FfMMXX+Tv78effhHlvnr+WVxzuPtpnh23UW5zyOfvL67ytf4HL59GubP6IK5ZUz5+m+Yoz7bZfY2V3//qQRyt6R8+inLt4/ycOP4iv//tX2ZrXf8637/HIRt/VVWbJvtW6cZ8r59xLKsKPz+aacb+NeMXHG4Ocbauvw5z+Vy5fp2f96YnN1Fuc5x9p6Z8IQEAAACwCI0nAAAAABah8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAW0afBtpq46KGdotw45jXbKc9O4W/t9l1c87Db59e/O0S59vUurrkehzhbJ3k/cR/eVjuMcc1xyt5/VdWwzt7B1M+4py4fK+uT7PrNw1Vcs31vnWffOs+u/+YmrtlM+bNqK5wDR/k7Hdf5s8rfVNUYrgHNNp/X6xnryq7LJss4ZfOfb2ruZ4yzMXwfTT4e2tt8PIy32XtuLrf59e/zsTOssv1jPOT31Ez5Ptfd5GtSnWe/YX+c7zO3l/mzOnp5E+WGq3ys1PP8vU7bLLs6y59/vXcWR9uH2Z7UruMjYbUXcbTG8LW2ZzPOhCf5b22aGTvNr+6i2HB1P+P6+fntEG7ffT/jTMg3vPjyNs5++eKrKHd3n+WqqsbDqzjbVrYubKfjuObd9jrODmP2rIYZJ7qjNs92M+oO4X4ffqZWVVWzyfeF7tFRVvPv5DWb753E2frhkyjW/dnruORwme2fVVVj+K3Y7Ga8gKt8X2z2Yd1Dvn4OY/5b1zO+f6uy39BW/k15NuX31bzK5nXX59dP+I8nAAAAABah8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCL6NDg2U1y0GcYsl1++xl1Ws6qqCcuOqyauWUf5/bcXXZTb1hDXXJ3F0WrfeRBnp19kvcfr2xf5DxgOcfQ2HCtTv45rnhyt4mw92kSx7r3TuGR/cRJnp1X2W4frfPxN+3xcbd4Iaz7Mr1/5VK22zXvfTbOPcsOcnzprXcme67rN5j/ftPvVNg9vw33mMGOMvc7nzm7aRbnNw7hkNe/kv7V9kK2JfZevnYe7/PnvXudzp726y3LP4pK1eZW/q8NVtifdPr+Oa75+/jrOjofsuZ6e52vH44t8nzt6nO1f7XE+/sZ+xt8tx2xRXq3yMXXojuJsf56f9far7KzTjpdxzZqxJzVNNgbGw4yifEN/yM+p0z7LTuNtXHM35mttEx6qVs19XHO9zdbkqqr9V1dRrrnP1+Q2/VCrqnHGBOqm7Dccuhnff+H6VVU1hmWn03z9njPVu+Nw/Xgnr9kOM57VXfb8p7O85vDmjHH1MjuX7V7m86/dZ98eVVXjIZ+D6WttjvJzQXuWfdNWVbVpNP/8za777ZYDAAAAgL+m8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAW0afBacyLtmF2bPKizWpGj6yd0mBes+/y7MMsdvTkJC453Q1xtntjE2eH21WUu/n5ZVzz6vY6zn46Zvf1qrmLa751dRRnfzC+GeXe6WaMlZt8rHQn2f03R/k7bfJhVfvLbK50M+bK9GY6/6r6sxlzoMnG6tTn1++O8jVoGrLl8tDk1+fXDZ/m83z7k6so11zdxDXboybOHn98nAU/Po9rjhcP4my6JO1X+RjvTvL52Bzv4+x4m61zw4t8n9vd7OLs69f3Ue7HX+b73GdfvYizzdWzKPdBf4hr/s72rTj73ju/F+WOvp+tsVVVzZTPlbHPBuswY+mc1jP25HCqVlXVWbbOT+t1XHK8vY2z/U02BsLtiP8fD97Iz4lvh+P3q8rPfmOTZ5vwA2yofK2/nvL1e/dsG+Xu/+h5XLM/uYiz7cMZh9qjcP7uZ6xf4XdKVdW0zeoOL2fUvMzfVf8yy7X3M8Zqn8+Vtsv22jZtFFRVdfm+NFxk6/LJSb5+H7p8Y7oJzxpVVeNNlu3fzMfq5rfyd9V9lG2MzfGM+RfwH08AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARfRpsB27uOi+naJc12S5qqpq8uvXNEaxfprRdxsOcbS5zbKHTZNf/1X8qmq4yp/r/S7LbQ/ZM62qerl9HWd/vnuZ1Zyu45pf3MbRenr3ZZT7h4eP4prvf/KdONs9Po9y43E+VjYXmzjbPDjJrt/nY6q7zufK4SQcgFU1Tuso16/z39oeVnF26LN30FQ+V/h10+c3efjyVRTrj7dxyebRjH3mzWyctedP4pLtaT52h3BP7rt8n5umfJ1pHmTzsapq3IT31e3jmndX+Tz78dVllPunn38R13z19adx9mj/Isq9CNeYqqrTNhv/VVUXf3oW5VaPT+OazYxs32T3NY35/XczpurwON8TV9/N7mv/Mts7q6rGJt8Tx/tsvWpmPCu+6eHbD+Ls+yfZ/Pm8y+fEeDiOs+np/3jGd9L1mO8Lt9fZmNx9mq2zVVXT73wdZ+ssP6cd7sJ5cZ/Pn+Eun79f/0l2pn3146dxzeNtfoZ54yRb6zb58lXV5d+fYzgFmhn/99LMuP60Cs8aUz7/mkM+Vo7ztkoN72Z1T/7Ow7jm6v3H+fVPs3m1aWdstgH/8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYRB8nm30cXfVZ2fEw5pefpjg7Hbrs+kN+/cOXQ5797D67/pe7uGbzOs8O+1WcnV7cZcHtNq7ZjvlYOQxZ3ddD+DuratzfxtndcBXlLp7HJevR26dxtu8eRrnVg3VcczjNr1+vsznQb5q45HiRj792yHvf3Sabg22bzf+qqhlTpfouXAO2+bPi102/fBlnVw+ydaZ7P58Pqz98I842T86j3HSSD7Kh8rFblY6zfI613Yw9sfI9uQ9/wtWY3/8XX+R7wo8//SLKffX8s7jmcPfTPDtm+9zmkB/JfnGVrzM/ePk0yp3VB3HNmvJx3TRHWa6dsc/E479q9SCO1vQPH0W59nE+r8ZfZPdfVbX9y1dRrn+dn7P4pvM38/H70ZtPotxfPn83rrk/vI6zbbgsHzX5nDga87XmcJ190+y+ehnX3H2e1ayq2pwd4uxhH+5L9/n+NVzl2eufvIhylz//ZVzzl6+zNaGq6sOT7Fvhoz/8TlyzfW8TZ7uHx1GueZ2P1ekq//7tzrP7H57mZ53a5WeNzZP8DNO+81aUax5kz7Sqan+U7zXHXbYG7sNcyn88AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwiD4NDmPeo2oPQxacprjmYUaPrN0eotzw1S6uOf74Ns42n2fZ8flVXLMbt3F22K7jbL/LnlVX+7jmydjE2SfNKsp9MWW/s6rqfsp/69f7LPv561/FNX/38lGcPVu/G+X6x+dxzeEkjlZ7n+Wa+/yZtt0mz67zeT3dZ2NgPIqXtepqjLPNPqy7ysc/v25/yNfZfpONnfE4Xw+7VT526jhbu4YZe1czY5+tLl8T45JTPnbHacZvbbMzwe4+PDtU1Ysv87Hy5YuvotzdfZarqhoPr+JsG+6f2+k4rnm3vY6zw5g9q6Hy93/U5tkurDs0XVyzzY+P1Wzyut2jo6zm38lrNt+bsSn/8EkU6/7sdV6Tb+jz43edNdmY+IPTN+Kazf3bcfbrfbbWrir/Tnijy8fvJtwX7l+GB8qquv5n+Vo7XOd7eHOczbX0/FBVNQ0z9sXjbK3vz/KzRnOXv6unz7PnevRZXvOs/V6ePcv2sPZhdn6qqhqOZ+w16Xnv6V1cczzKn1Wd5BtTs8rqNv2MM+Q2/6a5P4Tf/6v8XJbwH08AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARfRxspni6DR2aTKu2W2HOHt4ts9yf76Na06fvY6z45cvs+tv7+OaQ/6marw7xNlVm2U3Jw/imve3l3H2O4fsxr6oo7jm9XQVZ2/HXZR7djfGNb/44mmcffv5yyh3+vfej2seXeTPapfdfu2fpnO6qt3mz2p11MTZOl5FsWbGWlVTPrH2q+xh9TOWVX7dapoxdjbZmFz/8DSu2Vzk61w12XicMxqmJp8PQ2X334z535fGGddvx3xP3h2y33DYZnt3VVV/yPe5aZ9lp/E2rrkb8/NDU9m4XjX5mWC9vYuz+6+yPbG5z99p2+QjewzPet0045zXzdg7xnxPGMOy02k2/6uqDjO2pO44m9fjO3lNvmn4PD8nbtpsXl6cbeKav719I87+LDzTP5jy9ev7pydx9uQ8G2xNHcc1989fxNn+T/K5vrk4j3LNh1muqqp7Nz9Tdx+cZcHn+TdlO97E2dttttgMXz3Lr/8kvKeqqq+y64+fPIxL9if5WjuGR4j2Ii5ZXXjWq6pq38q/lerDbFz15/m8mvNNs30W7rd9vi8n/McTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBF9GuzGMS46tE2Ua/Zxydq9yLPjv9pm1/9xXnT66jK//s1ddv11/gDGffyqql3F0ere3US5o/5RXHN39TLOnm9votzbbfY7q6q+mtZx9lVlY2U37OKav7h+FWc/+fLLKHe++ySuOTb5u6rtFMWaLi9ZTTb/q6qaGetKt8l+xP6Q3VNVVTMd4mw/ZRNraPTz/13ta4iz3fokyh36fEFchWOsqmrqs3E+7vMx1vVzJlo2d8Y2H4/NkM/HOXO3abLs8Um+zz144yjOvt1nz+Cryp//OGNRbKbs/ofwnVZVXU/5+WH3LNvn7v/oeVyzP7mIs+3DbK7WUf7+p32+z4xjvq5M26zu8HJGzcv8XfUvs1x7P2et4Df1r/Jz2sPvZeP35Pc+imuePzuNs999dh7l1mM2z6uqTtpwTlbVejrLgvsZY3Kfr3X3c/bQQ/b91e+P45rNdf6sHlxkz+D6NP9OaWesi00bnumHfE3aPJrxXs+y+2qGGTWP8u+/2oX3/zj/Tqhw+FdVrT7If+t4lD2Ddsb/CO0O+b7Y3t1HuWnGuTy67rdaDQAAAAD+hsYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWEQfJ8cmjrZDlttfTnHN6Y9v4mz96ZdRbHj2Oi7Z3N7F2bbGKDcdZvT92vxZVd3nZcfjKNcfd3nN9WmcXfXrKPdkWMU1j4ejOPt6uo5y92P2Tquqvhiu4uzV7lUW3G3jmlOzj7N9k42rwyYuWdOQP6thn4/rsckWlqbN16o+XwFrHLLrt80hL8qvWT/KB1pzm42z8U/z+Tg+zNeO6VG2fo99vnb2+3xPmNbZOB/HcEOuqj6fOjWs5pwJsuzqLF/nH779IM6+f3IW5T7v8r1rPGR7Z1V+0Dpu8rFyPeZj5fY62z92n17GNaff+TrOVvheD3czBuB9nh3u8jX56z/ZRblXP34a1zze5vv3GyfZGrg5iUvyt1if52Oi+W62fqzfzdeP0+adODteZeOn7fOzX7vL19q6z64//Szfa8br/JtufZwf1FbZ9K06mXH/R/l9bbbZurRu8vX7wYx9sTkJv+ke5mfv9rv5Xtt+J5wDtzPaD1/P+abNcsOQz5Vmxlg5NHm2b7P9fp9/UlW7yc8Qw6Pst/aHOf2Hfzv/8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYRJ8Gx/shLjpNWe7wZ/dxzfqXX+XZL19GsWGXX78/bONs02QPYNWu45qH1SrP5q+qhq/vstwYD5VqjvLfWqujLLbLa67bLr9+OFa3bf5Qd30+VlYXY5QbVuEPraqjympWVU2PjqNcP+T3P0wznn+X31dNTZjLrz+ki1VVTW2YHfXz/111330nzk6vbqJcO+TvePc0Ww+rqvptuH6f5td/fZvP3e4oq9uf5uNx6vJs04fzsaqmsGw3Yz04fzPfEz5680mU+8vn78Y194fXcbYNX+tRkz/Toxl78uE6O+vsvnoZ19x9np+fNmeHKHfYz9gP7vPscJVnr3/yIspd/vyXcc1fvn4VZz88ydaVj/7wO3FNvul2t4+zD06zd7I+zs6zVVVjm8/f/o1s/tSMs++MpabasG7zg/B3VtU45tlpxo9Nl+XpR9dxzabNr7++zfald4/z93/34cM4e3nINpuz7834/v34Is62j7Nvimaf79/bLv+t49e7KNc+yM863Tp//+2Un+HGytaVrpkxrw75utY82WS5MculfCEBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFtGnwfGPX8VF90cnWfD/eR7XHL58Fmen2/sseNjnNdsmz66zx7qbUbPpZ1y/y7OH7SGreXMb1xxmPNdx00W5ps17pJvDUZw9m06z3IP8mX7vOw/j7MVvvxflVg+y31lVVf0mjjabKcq10zqvOebPqrohj4Z98l07xjX7Kf+tbWXZsc3HP7/hV3d59ihcO/LhUPWrbD5UVd3/30+j3OtdPsa/fhbuXVU1nWbZN//um3HN0w/O4+z6Ij4+VJ1k2RkrR/VXefasyfaEPzh9I67Z3L8dZ7/eZ2vXqrZxzTe6bPxXVW3Cde7+ZT7+rv/ZV3F2uM72j+Y4PDtWVb/JzwTTkI+s8Thbv/uzfPw3d/m7evo8e65Hn+U1+aZ/9Ud/HmffeZWdk9/7/XxNOP0wX2uHt7P1qwu/PaqqqpkxJ8LjX3uUnxOnQ74xNzPWur6yusNZvi9Pr/J1uZ+yNXTz7oO45vF5nn30ZjZW+t/Nx8rm0aM4ewj32jYfKrUJx39V1fAku6/uZMY39ZSPv7bP96UhHKtTP+P6U36Gja2+3XL+4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEX0aXD3r17HRdvuPgt+8SquOV3t4uxwuItyqyYuWe1R/KhqP2W5ZtrHNbt+E2eHKf+t426Mctsuf/7rJ1nNqqqLJ4+y4Pkbcc3fub2Os789XUW5Nx/mz/S3Pnknzl784LeiXP/gYVxz7Ls4u2qy3vMhrlg1tvn775t8Eg7tEOXaQ/6uKn9UNYZ9+iZfVvkN+1W4eFZVN2X7zDA+iGtOYz52d6ts/b7/VbbGVFX97C//Ks7+9P5plHv45/ne8Z3H78XZ739wHmcf/IO3o9zY5+9q+Dx/rps2GysXZ/mz+u1tvif97C5bQR9Mt3HN75+exNmT82xPauo4rrl//iLO9n+SrfObi3xMNR/m2e7dozz7wVkWfD7jTDzexNnbbbYGDl89i2vyTf/n538WZx+/uIxy3//sSVzzhx9/Emff/8N3o9zqD/LrT5t8rWumcF/s8/NcM+Oc2szYl5sH2W/Yv7mKa9ar7OxZVTVus2+l8S6/p9VFuCZVVT3O7qvv8/vf7Wf8j8oUfi10+fWnTT6uVm22h835pmma/F0N3YxxnR53h/zXTjO+qZrwt0758I/4jycAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABah8QQAAADAIvo0OL64y6veXGU1X93GJZthm2fHQ5br8r7bvpo4O3RdlOveyHJVVc35Ks7Wdoyj/SZ7Bqdj/qzW33ucX//diyj3pM+f1QeVvf+/Fo6VVf5MTx89iLOrJ+dRbtzMeP8zxnV6V9OU33/XrONsX7s4u9uHY6Cd4ppNl8/rwz7L9U1ek1+3+e/ejrNduH+MUz4fhnHG2D0Jx9lZPnd/8vxVnP35F8+iXPfseVzzV7/6NM7+1ef5u/rtn34c5d559524Zr/K3+vD751EuZPf+yiuef7sNM5+91m2zq/H/Jxz0mb3VFW1ns6yYLrGVlXt8z3hfp/ts90hP2f2++M421znz+rBRfYMrk/ztaI9io/a1YT7VzOEGxJ/qy/un8bZy/19lHtx9WVc8/WrbP2uquoe/d0o9/4H+ZxYv52P3+koy7X5klD5Ka1qmrGHj+F2e/TRJq65vc/PdM2fZ2fa8UU2pqqqhiY/F7QX2Vo/fJ6vH4cv82/17S5b604+eh3XXH8n2z+rqtpNNgin+3yvW035aJ3WQ5ztwr7Cfpf/1qnJr9834bP6lj9p/McTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBF9Gmxeb+Oi432WncbbuOY0TXE27aeN/SGuOByfxNn1UfZbmzfO45rTw+M4uxrHODuusyFw/MGjuGb74VGcrZMsu9nEQ7VO2ryfehiyZ9X2TVxzvcqzteqiWNdv4pLjjLkyTdlvbZv8npoun1f7YRVnK3tU1TT5+G/2337vfWpnvH9+Tf/WjLXjNMtOwz4u2ezyy9dNFnvW5vPx56t8nft6fB3l2u1VXPN2l9Wsqtrevoyzdf0iij1a/XZc8uSHH8fZ5rtnUW797mlc87R5J86OV9mZqO3zsdruZqyd6ZnsZ0NccrwOJ0BVrY+zcb2aM/9OZtz/UX5fm222fq+bfO94cJb/1uYkO+v1D+eciflN43AfZ+/GbEw8G+/yH/Asz07/JMv9/Xz5rvf/0Q/j7PqT7Pw/rPJ51nb5/GlnnOm79Kw65+x5ER4+q2oK99BmyBe78TJ/rrXLfuthxpm+unys7sPobpOf09v3Znx/32f338/4/q9xxvjLb6sO4U9o+/z6U5PvC2M4rOYMlYT/eAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBF9GhwvX8dFhxqymocsV1VVmymONmE/bXx4Etc8+uQ8zk5jVreb8vsfm+M4Wxd5dP3GKsp1H+fPqnmY1ayqOgzr7Pr9jPe/iYd1rdLWa5tff5zRz+3SaJNfv6Yujrbpo9rnl2+qibPDjNZ3W2OUG8e86JD/1OoqfAeHXV6UX/eT+zg6NOE4H/OXPN7l7+7+KhsPv3h6Hde8evoszt7tbqJcO+TX385YZu7bQ5w9ussKf/L0SVxz8/EHcfbBabbPrI+P4ppjvHhW9W+Ez6rN1+5mxtrVhnWbH+TvdBzz7BT+2EN+zKzpR/m4btr8Ya1vs/PLu8f5+7/78GGcvTxk+9zZ9/K1km963J/G2XSkT+EZpapqd8jf39NnfxXlfvrP84Pa0U1+/Uf/+PeiXP/DR3HN1Xn+ndDMWBcPY/ZdNVzl31/T59s4267D0fLWJr/+bs4ZJhsDzYwz/f1N/qz2d9mzWj/K3+l6xvfPNpyD3YzvhGbG9992xnlzNWV7yJy9tg759Zs+e1btt/w/Sv7jCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvQeAIAAABgERpPAAAAACxC4wkAAACARWg8AQAAALCIPg3eb+/yqkddFOveyHJVVav+KM42m6yf1j7ZxDXbTx7H2fGLJsoNX17HNbub/PmPp9n1q6rG6+xZde0Q16zvn8TR5jBGuf3PtnnNj/Nx1b6Z/dZmyn5nVVU3p53bZOGpprhkf8jf/9Bk9zX1M57p7hBnq83rjmG0qfz6fVq0qnZd9g6aJq/Jb7jK3910uo9yzdEqrjke8nVu2GVr0u2vXuXXf/k0ztZ4H8W204y1e8yzY2XPv6rqJ/fZs7r41U/jmq/+KN+/33mVjav3fv/tuObph+dxdng7O7906/hIVtXM2OfXWa49CoNVNYV7d1VV02VrYl95zeEsH6vTq/z80E/ZvNq8+yCueXyeZx+9mY2V/ndnjBW+4b9/74dx9tV9tve/OuRn+u2M+dOH2d3VTX79H/0szt702Vp7tvs4rjn9wbt5djNjrm+ztX64nLEvtjO+1T7KzhvTPt+/xqf5Xttss/vavtrFNes2WxOrqpopW2tX9/k5+XCf3/9qdRzlxnz7rGnMv7/aNl+Xpz4bq+00Y6+fsS2MXVZ3ym8/4j+eAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYhMYTAAAAAIvo02Az3cdF1w9Ps4v/9uO4Zvf4JM4O6ya7/nEX1zycP4iz9XQbxdphiEtOU55tb/J+4nS4iXL7++yeqqrar/Ps4Y3wuT6/zq9/m7/X6ftjFnwrH3/N6RRn09bvFP7Mqqpdk4e7yuZKO865p/z5N+2MOTBm2b7Jrz+02f1XVXXhIxjCZ8rfYs5A32cvpL+It7k6HOfZ2mbZh4cv45JPhn2cvQsf1bbyudvM2Ge2Qz7O9+H68S+ufxXX/OXn+Vh5/OIyyn3/sydxzR9+/Emcff8P341yqz/Irz9tNnG2SedVn7/Tpp+xzo/Z9ZsHM8bUm6s4W6/ycT1ud1kunYBVtbo4i7P1OLuvvp9x/3zD7/zgv4yzr6+yc/LNTbbOVFVdXeXfVFdX2fn34TDjnHyfX7/7+lmUG15exDXrbkb2KJ8//TpbF6f3snleVVXvPoyjzZQd6qddvn6Mn+XfVPUX2Vo3dHnN3ZSfS9qTcA3v8v1rep2fYcbwk7KZ8Ukzznj81ebjaurD8+aMT4qpnfGswtfadzPO5QH/8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYRB8nT5s42r51EuW6Jw/jmv1Hx3G2eec0yrWr/J6aV3mPbpqG7Pptl9dsxjg7dnndfjdlwRf7uObh/ibOdpeHKDf24e+sqmab1ayqmiobA+NdXLKmD4/i7Oosy7VNPlWbLh8rNWXjupnx/MfKr982+bxqwmeQ/9Kqps3HSo2bKNZ19zN+Af+mw0m+dtUuG2f3L3Z5zbt8T+h+eR3lPtpn+0FV1e0q3+eqfxTFfjbki9euZsyHOabsHdwc8rXji/uncfZyn83JF1dfxjVfv3oWZ7tHfzfKvf9B/v7Xb6/j7BRuSe2crSOP1hTuM+Mqr3n0UbYeV1Vt72ec9f48G6vji3ydH5pXcba9yA4Fw+f5mYxvOl0/iLNn72XZ7fqtuObLX2X7R1XV3WW21m12L+OaR6fZd1JVVffoPLv+m/mcPMw4+3XbfLVZTWF2M+M7acY3VRd+/w2b/P6Hj/P1a7jM9vsuPM9WVTVNvtaMN9n17/Pts+qzfK6sH2V7aHuUv9NxP+NcOM74/gxfQTtjX2zDb9qqqq7PxuC3vdP4jycAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABbRp8HmnQdx0fadsyjXvbfJa350EmfrOKvbTPHtV11OcXTa7aLcYRrjmu0hjlY3ZNevqhr6dVZzyu+/vR/ibN9lv3U4X8U1d0N+/brcRrG2y99Vt8qf1XiUjev2eEbNoYmzXRidMVSrafN+9rDP67ar7EeMYxfXnPb5uOpW4SQ85DX5dd27x3F2+Pwuym3W+XjcP46jdfQ6q/vWG/k+190+ibNvTNna9a+v8vv/bPtlnH0x3sTZ3ZityW3la/c43MfZuzFb6J6N2Zj663Cenf5Jlvv7r/PLv/+Pfhhn1588inLDKn/+bZePqzbcE7om37tqmLHOXszYE3bZS2hmnLPGyxlnkl32Ww9znhXfsDnN1+XhkO39m+Yorvn43Rlnj/ez76/uLP9Q6M7y6zcXp1nN97N1pqqqP87Xj2bG+fuQnilv8zN1nefrx9iG35UzDtWbh/m36v13Hka57mV++G7P83NZl+7h44zn//OXcXT/dvZN1Z9mY/qvw3l0nDGuu032rA4zfkDbztgXDtk76L/lTxr/8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBFaDwBAAAAsAiNJwAAAAAWofEEAAAAwCI0ngAAAABYRJ8Gm/YoLtqcHEe5dhNfvsb7GdmrKQsO27hmt897dMPDJsqtt6u85s0QZ8cpu35VVVOH7Ppd/vy7/PI1HvZRbtrlNTdHM55reFuHXTimqqo5ZM+0qmo9ZnWbQ/5QN10+VrdDNq6aGS91mmY8q77L6w5jlGvb/P6nfs68ynJNk9fkN3ydz53pED7ncIxXVTXtSZztP8n2uTcePo5rbj5e59m/yK5/8dP8nn54yH/rj29fxNkvbp9HuXF3F9dspnxPSkfVVNkaU1W1O9zH2afP/irK/fSfZ/thVdXRTX79R//496Jc/8NHcc3Veb7PNm22zh/GfK4OV3l2+jw/67XrcLS8tcmvv8v3z/EuGwNNPlT4W3TvPMjD99n4WZ/k60e9lc+1Ljz/HfLPtFq/eRpn95vs+s3RjMP/asb8mXFObNpwXbifsdbc5+91dZyti+Fxtqqqxhln6u48OxeMD/LB0u1nrLWH8Awz4/t/f8gf1uHTl1FuenUb12z3+WJ7+p/l563m97M1YGpmnIvb/AzZhNNqqHz8JfzHEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABah8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARfRo8vLyOizZPz6Lc7uI0r7m9j7P9aopy08VJXHO6GOPs6r9+I8qNP5tx/Z+8jrP1Kn9W07DPgofsmVZVjTP6me2QPdfhfohrThf59ZvfXWU126O45uqtLs62q3AKdvnzH6Ymzvbh5adxxvufcf1uxn3t2/S55nO18p9a0yEsGf9OftPwk8s4262ylzec5XO3n7HO1FH2ntuPH8Ulz773IM4efyfbZx9/+iSu+cFVvs/88DqcEFX1/PmLKHf//Iu45mG7i7Ov7rN15tUhP+dsD/k604fZ3dVNfv0f/SzO3vSbKHe2+ziuOf3Bu3l2k82raZuPqeFyxlxt8/fafRSeCfbZM62qGp+G56yqarbZfW1f5eOfb1qfxZ8/NXyUvevmUTZ2qqrai/z6h6+yvaafsSYOr/Kz1+qN7L6adX72HroZ59Rmxv9IdOG6fJSfC8YZ35/DPlzD2nysjJczDqovs9/avZNf/3jIs/fX2VzZz1jr21X+/qefZWtt8/Iqrnl/l+/L6zb/rl99cBzlxjfzc2F3l+81w0n2Xtvh2/0fJf/xBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwCI0nAAAAABah8QQAAADAIvo0eFh3cdGh2Ua51X6Ka46HQ5zdnWe3tTqNS1a3znt0h7bJap6exDWn4/z515++zOveH0W5bj/GNcchf1dj2PvMr17VXsTDuvoPz7Oa6+w5VVUNq/z6QzZUqmny8dc1+fM/VPYD+jF/A30X3lRVTTN+a1fpHMjXlSaP1qEJ5/W4z4vya8Zf5s9uPM7exyrbjqqqasiWg6qqmo7D4MN1XLM/z7P17kUUe/Ao32dOL2/j7NkX+dx99PCdKDc8+jCuud/dx9nXVzdR7ubmMq55dZVf/+rqOso9HLJcVVV7n1+/+/pZlBteXsQ1625G9ugsivXrTVxyem+XX//dh3G0mbK9dtrNOBN9NmMR+oshig3djJp8w2HKnnNVVds/iHLNwxlnv82Mtf5Jttc1T/Mx2Vzn6/f0NJtr43F+T8PpjDNlO2Outdn87WZ8Uo1Dvi41Y/as9jczxt8X+VyfTrIzVLPO39V0m//W4Sa7/t0v83va3+dj9egq+63jNt8/121+/9N9fl/DXfYb1pWPv3GzirP9lH0AjXM+lAL+4wkAAACARWg8AQAAALAIjScAAAAAFqHxBAAAAMAiNJ4AAAAAWITGEwAAAACL0HgCAAAAYBEaTwAAAAAsQuMJAAAAgEVoPAEAAACwiGaapuk/9I8AAAAA4D8+/uMJAAAAgEVoPAEAAACwCI0nAAAAABah8QQAAADAIjSeAAAAAFiExhMAAAAAi9B4AgAAAGARGk8AAAAALELjCQAAAIBF/L+rlXFaX8/n2wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocess dataset DONE\n",
            "\n",
            "\n",
            "-- UC1_NI EMBEDDING--\n",
            "\n",
            "Embedding images using resnet50 encoder\n",
            "Load pretrained Resnet50 offline from weights path: ./resources/pytorch_model.bin\n",
            "Generating embeddings for ./resources/processed_dataset_test/ST_pred_results/UC1_NI.h5 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 107/107 [00:13<00:00,  7.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding shape (X_test): (13649, 1024)\n",
            "\n",
            "-- UC1_NI REGRESSION PREDICTIONS--\n",
            "\n",
            "Loaded model from ./resources/processed_dataset/ST_pred_results/split0/model.pkl\n",
            "Loaded model from ./resources/processed_dataset/ST_pred_results/split1/model.pkl\n",
            "\n",
            "-- UC1_NI PREDICTION DONE\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "prediction"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-5f2674e1-afbd-4b32-9545-0d2c2e60bc13\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cell_id</th>\n",
              "      <th>gene</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65800</td>\n",
              "      <td>A2M</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>65810</td>\n",
              "      <td>A2M</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65835</td>\n",
              "      <td>A2M</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>65844</td>\n",
              "      <td>A2M</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65863</td>\n",
              "      <td>A2M</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278535</th>\n",
              "      <td>65465</td>\n",
              "      <td>ZEB2</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278536</th>\n",
              "      <td>65475</td>\n",
              "      <td>ZEB2</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278537</th>\n",
              "      <td>65513</td>\n",
              "      <td>ZEB2</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278538</th>\n",
              "      <td>65515</td>\n",
              "      <td>ZEB2</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278539</th>\n",
              "      <td>65518</td>\n",
              "      <td>ZEB2</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6278540 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f2674e1-afbd-4b32-9545-0d2c2e60bc13')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5f2674e1-afbd-4b32-9545-0d2c2e60bc13 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5f2674e1-afbd-4b32-9545-0d2c2e60bc13');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b200e354-0ee5-4d5d-9833-9fede154b1af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b200e354-0ee5-4d5d-9833-9fede154b1af')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b200e354-0ee5-4d5d-9833-9fede154b1af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2960a613-ad14-4485-8185-41417c2a9d89\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('prediction')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2960a613-ad14-4485-8185-41417c2a9d89 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('prediction');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         cell_id  gene  prediction\n",
              "0          65800   A2M        0.51\n",
              "1          65810   A2M        0.64\n",
              "2          65835   A2M        0.46\n",
              "3          65844   A2M        0.72\n",
              "4          65863   A2M        1.05\n",
              "...          ...   ...         ...\n",
              "6278535    65465  ZEB2        0.02\n",
              "6278536    65475  ZEB2        0.35\n",
              "6278537    65513  ZEB2        0.48\n",
              "6278538    65515  ZEB2        0.02\n",
              "6278539    65518  ZEB2        0.46\n",
              "\n",
              "[6278540 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = infer(\n",
        "    data_file_path=\"./data/UC1_NI.zarr\",\n",
        "    model_directory_path=\"./resources\"\n",
        ")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5w-KSO52hDj"
      },
      "outputs": [],
      "source": [
        "# This command is running a local test with your submission\n",
        "# making sure that your submission can be accepted by the system\n",
        "# crunch.test(\n",
        "#     no_determinism_check=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47HkOHSSynG5"
      },
      "source": [
        "**You can download this notebook and then submit it at https://hub.crunchdao.com/competitions/broad-1/submit/**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kppG1inFIopT"
      },
      "source": [
        "Steps to submit the notebook:\n",
        "\n",
        "- Download locally the pre-trained model Resnet50 `pytorch_model.bin`: https://huggingface.co/timm/resnet50.tv_in1k/tree/main (because internet is restricted on CrunchDAO submission platform)\n",
        "- Create a directory named `model`\n",
        "- Place the file `pytorch_model.bin` inside the newly created `model` directory\n",
        "- Submit the `model` directory in the \"Model Files\" field\n",
        "- Run notebook in the cloud with CPU environnment\n",
        "- Set Training to Yes\n",
        "\n",
        "![notebook_submit_steps](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/notebook_submit_steps.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}